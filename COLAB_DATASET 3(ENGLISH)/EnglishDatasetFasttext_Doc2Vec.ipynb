{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EnglishDatasetFasttext&Doc2Vec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rjPzL06mgZk","executionInfo":{"status":"ok","timestamp":1632593391417,"user_tz":-330,"elapsed":1311,"user":{"displayName":"Anisha Dayma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFOxzEXKGnjnkBeBDKDYB_LF9O6VPgAIN1ZNTs=s64","userId":"01419279134853010731"}},"outputId":"788f71b9-5349-40c2-ac2b-cd994e9feaf9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"BatH6iignBBS","executionInfo":{"status":"ok","timestamp":1632593242412,"user_tz":-330,"elapsed":1159,"user":{"displayName":"Anisha Dayma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFOxzEXKGnjnkBeBDKDYB_LF9O6VPgAIN1ZNTs=s64","userId":"01419279134853010731"}}},"source":["path = '/content/gdrive/MyDrive/HateSpeechACM/englishdataset.csv'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"blTkmCDkoGDz","executionInfo":{"status":"ok","timestamp":1632593244701,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anisha Dayma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFOxzEXKGnjnkBeBDKDYB_LF9O6VPgAIN1ZNTs=s64","userId":"01419279134853010731"}},"outputId":"7b218737-746e-453e-c9aa-d4c903d0076b"},"source":["print(path)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/HateSpeechACM/englishdataset.csv\n"]}]},{"cell_type":"code","metadata":{"id":"6MRCdCGToMKO","executionInfo":{"status":"ok","timestamp":1632593248555,"user_tz":-330,"elapsed":343,"user":{"displayName":"Anisha Dayma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFOxzEXKGnjnkBeBDKDYB_LF9O6VPgAIN1ZNTs=s64","userId":"01419279134853010731"}}},"source":["import pandas as pd\n","url='/content/gdrive/MyDrive/HateSpeechACM/englishdataset.csv'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":433},"id":"YIbpJLkhoXdQ","executionInfo":{"status":"error","timestamp":1632593251128,"user_tz":-330,"elapsed":625,"user":{"displayName":"Anisha Dayma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFOxzEXKGnjnkBeBDKDYB_LF9O6VPgAIN1ZNTs=s64","userId":"01419279134853010731"}},"outputId":"bba754a2-8c06-4e89-a086-fcbe6b4aca18"},"source":["import os, shutil\n","import pandas as pd\n","dataset=pd.read_csv(url)\n","dataset"],"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e8ba1fa52f24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/HateSpeechACM/englishdataset.csv'"]}]},{"cell_type":"code","metadata":{"id":"HxhfQqQIpp-8"},"source":["dataset.dropna(subset = [\"tweet\"], inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLyif_18pubI","outputId":"9acbc03c-04b7-44ce-b93f-d8713bb6380b"},"source":["dataset[\"class\"].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count     1153\n","unique       2\n","top        NOT\n","freq       865\n","Name: class, dtype: object"]},"metadata":{"tags":[]},"execution_count":78}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kiN4ner4p0W0","outputId":"65c155dd-89c9-411e-9da0-b8d83cb4e420"},"source":["pd.unique(dataset[\"class\"])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['NOT', 'HOF'], dtype=object)"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L47_K4THp2p0","outputId":"e767a5cd-739a-4d98-f245-800729c9cfbf"},"source":["dataset[\"class\"].describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count     1153\n","unique       2\n","top        NOT\n","freq       865\n","Name: class, dtype: object"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4R9DHGup-QQ","outputId":"b339f2b9-c83c-45eb-ac47-885774ca17ed"},"source":["# Adding text-length as a field in the dataset\n","dataset['text length'] = dataset['tweet'].apply(len)\n","print(dataset.head())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                               tweet class  text length\n","0  West Bengal Doctor Crisis: Protesting doctors ...   NOT          128\n","1  68.5 million people have been forced to leave ...   NOT          117\n","2  You came, you saw .... we will look after the ...   NOT           62\n","3  We'll get Brexit delivered by October 31st.   ...   NOT          146\n","4  Fuck you. Go back to the dark ages you cow @IB...   HOF          119\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":243},"id":"9GYhE3IPqER9","outputId":"5a38d514-2398-4a3b-802c-6f62972a63fb"},"source":["#Basic visualization of data using histograms\n","# FacetGrid- Multi-plot grid for plotting conditional relationships\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","graph = sns.FacetGrid(data=dataset, col='class')\n","graph.map(plt.hist, 'text length', bins=50)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<seaborn.axisgrid.FacetGrid at 0x7fb1bcf60710>"]},"metadata":{"tags":[]},"execution_count":83},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x216 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"7QbSrdyUqMG5"},"source":["tweet=dataset.tweet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDamvF7UqRvB"},"source":["# remove special characters, numbers, punctuations\n","dataset['tweet'] = dataset['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RSPpiY--qVcc","outputId":"136db3ae-b07d-41cb-bd74-33ff303d365c"},"source":["dataset['tweet']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       West Bengal Doctor Crisis  Protesting doctors ...\n","1            million people have been forced to leave ...\n","2       You came  you saw      we will look after the ...\n","3       We ll get Brexit delivered by October   st    ...\n","4       Fuck you  Go back to the dark ages you cow  IB...\n","                              ...                        \n","1148    Each of the divorce must be pronounced with a ...\n","1149    He is    yr old Harswaroop Chauhan  His only s...\n","1150    Peace of graveyard or peace of mind   asadowai...\n","1151    The humble #tribute on the sacrifice day of #J...\n","1152    Happy Birthday to Sh  nitin gadkari Ji  God bl...\n","Name: tweet, Length: 1153, dtype: object"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"code","metadata":{"id":"cUZfTo__qj19"},"source":["#Removing Short Words\n","dataset['tweet'] = dataset['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsF_zo8eqmAG","outputId":"8672ff26-a3ef-4c82-c639-820be4082853"},"source":["#Tokenization\n","tokenized_tweet = dataset['tweet'].apply(lambda x: x.split())\n","tokenized_tweet.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [West, Bengal, Doctor, Crisis, Protesting, doc...\n","1    [million, people, have, been, forced, leave, t...\n","2          [came, will, look, after, fort, Good, luck]\n","3    [Brexit, delivered, October, Help, build, move...\n","4    [Fuck, back, dark, ages, IBNLiveRealtime, Rape...\n","Name: tweet, dtype: object"]},"metadata":{"tags":[]},"execution_count":88}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEQN1aD1qrr4","outputId":"1f90c48a-6847-4668-88a9-f5af12cd401b"},"source":["#Stemming\n","from nltk.stem.porter import *\n","stemmer = PorterStemmer()\n","\n","tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n","tokenized_tweet.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [west, bengal, doctor, crisi, protest, doctor,...\n","1    [million, peopl, have, been, forc, leav, their...\n","2          [came, will, look, after, fort, good, luck]\n","3    [brexit, deliv, octob, help, build, movement, ...\n","4    [fuck, back, dark, age, ibnliverealtim, rape, ...\n","Name: tweet, dtype: object"]},"metadata":{"tags":[]},"execution_count":89}]},{"cell_type":"code","metadata":{"id":"WuKEWjWk2KFJ"},"source":["dataset[\"class\"]=dataset[\"class\"].replace(to_replace =['NOT'], value =0)\n","dataset[\"class\"]=dataset[\"class\"].replace(to_replace =['HOF'], value =1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGzu4Top6DBJ","outputId":"d58a0188-637b-47c3-87a0-d628a3594dbd"},"source":["dataset['class']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       0\n","1       0\n","2       0\n","3       0\n","4       1\n","       ..\n","1148    0\n","1149    0\n","1150    0\n","1151    0\n","1152    0\n","Name: class, Length: 1153, dtype: int64"]},"metadata":{"tags":[]},"execution_count":91}]},{"cell_type":"code","metadata":{"id":"F0arF6JprZTy"},"source":["X=dataset['tweet']\n","y=dataset['class']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4ZdRsXfqzlD","outputId":"78f6e789-e445-4129-de68-daf493d51285"},"source":["import sklearn.model_selection as model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.65,test_size=0.35, random_state=101)\n","print (\"X_train: \", X_train)\n","print (\"y_train: \", y_train)\n","print(\"X_test: \", X_test)\n","print (\"y_test: \", y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train:  361                  back England miserable excuse human\n","134    absolutely right that government does support ...\n","621    Stop lying genuinely attach huge importance Go...\n","155    akashbanerjee video which lists points underst...\n","737    friend sends text asking does risk losing inst...\n","                             ...                        \n","575    VtvGujarati isudan gadhvi golanihemant patelke...\n","973    Under Islamic divorce pronounced twice which o...\n","75     Basically Mamata just tags protest against wha...\n","599    This zomato these people give gyaan twitter ab...\n","863    What happened Delhi happened Parliament Happen...\n","Name: tweet, Length: 749, dtype: object\n","y_train:  361    1\n","134    0\n","621    0\n","155    0\n","737    0\n","      ..\n","575    0\n","973    0\n","75     1\n","599    0\n","863    0\n","Name: class, Length: 749, dtype: int64\n","X_test:  686     Boris Johnson faces Supreme Court make stand t...\n","821     century realDonaldTrump congratulate Twitter w...\n","407     Scottish Conservative leader RuthDavidsonMSP w...\n","1086    those disbelieved #IndianArmy PMOIndia claim a...\n","1111            They have sold their soul dare them reply\n","                              ...                        \n","1126    instructions #AIMIM President asadowaisi Sahab...\n","341     like that chanting Allah Muslim will make chic...\n","334     Where were when indian Muslim celebrate Pakist...\n","555     serving halala meat person practicing Islam th...\n","353     disgrace real America Also women Imammy Americ...\n","Name: tweet, Length: 404, dtype: object\n","y_test:  686     0\n","821     0\n","407     0\n","1086    0\n","1111    1\n","       ..\n","1126    0\n","341     0\n","334     0\n","555     0\n","353     1\n","Name: class, Length: 404, dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hAFa9bkmn3Qk"},"source":["Fasttext"]},{"cell_type":"code","metadata":{"id":"E4aPTiLusLGy"},"source":["from gensim.models import FastText"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_iGFndKsZbP","outputId":"ce553517-2a9f-4734-95ed-8c62dbe91639"},"source":["\n","print(\"Training a Gensim FastText model\")\n","model = FastText(sentences=tokenized_tweet, size = 200, window = 2) # workers = workers, \n","print(\"Training complete\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training a Gensim FastText model\n","Training complete\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRi1NqPB3Gf_","outputId":"26e86bff-b0cc-4ca4-e172-f3576a4e7d47"},"source":["model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<gensim.models.fasttext.FastText at 0x7fb1bc614510>"]},"metadata":{"tags":[]},"execution_count":96}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uiw8W_zys1bK","outputId":"96769e11-d90d-4ab9-b4ad-184a8b1baadd"},"source":[" model.most_similar(positive=\"violence\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('violenc', 0.9991224408149719),\n"," ('violent', 0.998301088809967),\n"," ('cricket', 0.9980921149253845),\n"," ('#tripletalaq', 0.9980901479721069),\n"," ('instant', 0.9980819225311279),\n"," ('same', 0.9980736970901489),\n"," ('come', 0.998069167137146),\n"," ('#tripletalaqtruth', 0.9980635643005371),\n"," ('wouldn', 0.9980567693710327),\n"," ('break', 0.9980558753013611)]"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"code","metadata":{"id":"37rbUHxwtXYK"},"source":["def word_vector(tokens, size):\n","     vec = np.zeros(size).reshape((1, size))\n","     count = 0.\n","     for word in tokens:\n","         try:\n","             vec += model[word].reshape((1, size))\n","             count += 1.\n","         except KeyError: # handling the case where the token is not in vocabulary\n","                        \n","             continue\n","     if count != 0:\n","         vec /= count\n","     return vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZdmO1GF-l7x2","outputId":"4c440c28-73b4-4754-bf78-87722927da48"},"source":["model.most_similar(positive=\"hate\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[('them', 0.9998879432678223),\n"," ('their', 0.9998869895935059),\n"," ('that', 0.9998803734779358),\n"," ('there', 0.9998785853385925),\n"," ('they', 0.9998767971992493),\n"," ('these', 0.9998754262924194),\n"," ('thi', 0.9998748302459717),\n"," ('then', 0.9998742341995239),\n"," ('india', 0.9998741149902344),\n"," ('state', 0.9998732805252075)]"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCTCBudOmCdD","outputId":"8f72ab76-4355-45a8-beb1-a1a20a6ec9bf"},"source":["model['violence']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \"\"\"Entry point for launching an IPython kernel.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([ 0.01394737, -0.02017253,  0.00708241, -0.00519892,  0.01045253,\n","       -0.00212145, -0.02321891, -0.00374098, -0.01381655,  0.01768276,\n","       -0.03570081, -0.00273025,  0.00962313, -0.0414643 ,  0.01193833,\n","        0.0121599 , -0.01792444, -0.00129876,  0.0190659 ,  0.02227197,\n","       -0.00702816,  0.00358107,  0.02813074, -0.00269868,  0.00495798,\n","       -0.0046413 , -0.02974348,  0.00094767,  0.03298869, -0.00216247,\n","       -0.00350348, -0.01472243,  0.00140327,  0.02555923,  0.01295226,\n","       -0.00981752, -0.00471557, -0.01077412, -0.00685683,  0.01819493,\n","       -0.03307827, -0.01464627, -0.02082243,  0.0062248 ,  0.02107238,\n","        0.00617152, -0.00834599,  0.00717395,  0.00671348,  0.02110045,\n","       -0.01148926, -0.00541689, -0.02898843,  0.00796483,  0.00584706,\n","       -0.02231819, -0.00693626,  0.00395758, -0.00164627, -0.0273523 ,\n","        0.00166833,  0.00645697,  0.00369149, -0.00883837,  0.01025361,\n","        0.00787974, -0.02162786, -0.02180847, -0.01656111,  0.01073896,\n","       -0.0137963 ,  0.00242667,  0.0038141 ,  0.01193912,  0.01615116,\n","        0.04697153, -0.01568608, -0.01582216,  0.00055454,  0.01089363,\n","        0.0158653 , -0.00169312, -0.03459915, -0.03172687, -0.02879482,\n","        0.0111648 ,  0.00125987,  0.01260291,  0.02769993, -0.0162619 ,\n","       -0.02275194,  0.00313064, -0.00217566, -0.01472583, -0.00032928,\n","       -0.00949206, -0.00865475,  0.02322058, -0.02420914, -0.00660404,\n","       -0.00784571, -0.00408864,  0.00832193,  0.01590642,  0.01755637,\n","        0.02441804, -0.00738452,  0.0041016 ,  0.0091046 ,  0.02042668,\n","        0.0051382 ,  0.01660879,  0.01560635, -0.00783596, -0.01622122,\n","        0.01558783, -0.02492439,  0.01054651, -0.00261549,  0.00332417,\n","       -0.02058339, -0.00937661,  0.00616805, -0.00705306, -0.00914994,\n","       -0.00727451, -0.00573905,  0.00185717, -0.01109877, -0.00411312,\n","       -0.00866239, -0.00912503, -0.02168281,  0.03957522,  0.00246862,\n","       -0.00457067, -0.02406367, -0.00696813, -0.01025244,  0.01396913,\n","        0.01976818, -0.01337453, -0.02212962,  0.02869336, -0.03004117,\n","        0.01553921, -0.00794564, -0.00353418, -0.04140105,  0.0182885 ,\n","       -0.00754256,  0.00577207,  0.01523579,  0.00784291,  0.01045218,\n","       -0.00178743, -0.01057616, -0.00964232, -0.00040711, -0.00061507,\n","        0.01170449, -0.02022589, -0.00189568, -0.01460124,  0.00278744,\n","        0.00982664,  0.02112542, -0.00366918, -0.01783787, -0.00788462,\n","       -0.00493243,  0.00547701,  0.03077677,  0.00026588, -0.02794066,\n","        0.00471589,  0.01271991,  0.02891147, -0.00628257,  0.01468709,\n","       -0.00617211,  0.00027724,  0.00129523, -0.00177244, -0.0007793 ,\n","       -0.00209967,  0.00404645, -0.02424109, -0.00600465, -0.02661265,\n","       -0.00173782, -0.01731949,  0.02420882,  0.001631  ,  0.0157502 ,\n","        0.00726075, -0.01305794, -0.02171372, -0.02088423,  0.02399467],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z0Ew8rCRmw9s","outputId":"0c95bb76-e546-4748-840a-e56acb03a467"},"source":["semantically_similar_words = {words: [item[0] for item in model.most_similar([words], topn=5)]\n","                  for words in ['hate', 'reservation','pakistan','hindu', 'muslim']}\n","\n","for k,v in semantically_similar_words.items():\n","    print(k+\":\"+str(v))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["hate:['them', 'their', 'that', 'there', 'they']\n","reservation:['reserv', 'nation', 'with', 'that', 'india']\n","pakistan:['#pakistan', 'that', 'thi', 'they', 'stand']\n","hindu:['thing', 'indian', 'where', 'there', 'them']\n","muslim:['that', 'there', 'them', 'then', 'these']\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Byl08kCdts5Y","outputId":"93954cc5-ff89-4b1b-b332-4b4b660e0b1d"},"source":["import numpy as np\n","fasttext_arrays = np.zeros((len(tokenized_tweet), 200))\n","\n","for i in range(len(tokenized_tweet)):\n","    fasttext_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n","    \n","fasttext_df = pd.DataFrame(fasttext_arrays)\n","fasttext_df.shape"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  \n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["(1153, 200)"]},"metadata":{"tags":[]},"execution_count":100}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"u6wqtA2D58h1","outputId":"99db89e3-f633-4074-f700-6d5918060631"},"source":["dataset"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>class</th>\n","      <th>text length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>West Bengal Doctor Crisis Protesting doctors a...</td>\n","      <td>0</td>\n","      <td>128</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>million people have been forced leave their ho...</td>\n","      <td>0</td>\n","      <td>117</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>came will look after fort Good luck</td>\n","      <td>0</td>\n","      <td>62</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Brexit delivered October Help build movement t...</td>\n","      <td>0</td>\n","      <td>146</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Fuck back dark ages IBNLiveRealtime Rapes happ...</td>\n","      <td>1</td>\n","      <td>119</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1148</th>\n","      <td>Each divorce must pronounced with stipulated i...</td>\n","      <td>0</td>\n","      <td>118</td>\n","    </tr>\n","    <tr>\n","      <th>1149</th>\n","      <td>Harswaroop Chauhan only sibling Gangaram been ...</td>\n","      <td>0</td>\n","      <td>278</td>\n","    </tr>\n","    <tr>\n","      <th>1150</th>\n","      <td>Peace graveyard peace mind asadowaisi asks Ami...</td>\n","      <td>0</td>\n","      <td>63</td>\n","    </tr>\n","    <tr>\n","      <th>1151</th>\n","      <td>humble #tribute sacrifice #JhansiKiRani #Laxmi...</td>\n","      <td>0</td>\n","      <td>208</td>\n","    </tr>\n","    <tr>\n","      <th>1152</th>\n","      <td>Happy Birthday nitin gadkari bless good health...</td>\n","      <td>0</td>\n","      <td>84</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1153 rows Ã— 3 columns</p>\n","</div>"],"text/plain":["                                                  tweet  class  text length\n","0     West Bengal Doctor Crisis Protesting doctors a...      0          128\n","1     million people have been forced leave their ho...      0          117\n","2                   came will look after fort Good luck      0           62\n","3     Brexit delivered October Help build movement t...      0          146\n","4     Fuck back dark ages IBNLiveRealtime Rapes happ...      1          119\n","...                                                 ...    ...          ...\n","1148  Each divorce must pronounced with stipulated i...      0          118\n","1149  Harswaroop Chauhan only sibling Gangaram been ...      0          278\n","1150  Peace graveyard peace mind asadowaisi asks Ami...      0           63\n","1151  humble #tribute sacrifice #JhansiKiRani #Laxmi...      0          208\n","1152  Happy Birthday nitin gadkari bless good health...      0           84\n","\n","[1153 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"SJW_JItqIMue"},"source":["#def wv2centroid(fasttext_df):\n","#   return np.mean(fasttext_df, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqLB8Gk7IySb"},"source":["#fasttext_df['mean'] = fasttext_df.mean(axis=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSKBvfF5QovF"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score\n","\n","from sklearn.metrics import classification_report,confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix, accuracy_score\n","train_ft = fasttext_df.iloc[:1153,:]\n","test_ft = fasttext_df.iloc[1153:,:] \n","xtrain_ft, xvalid_ft, ytrain, yvalid = train_test_split(train_ft, dataset['class'], random_state=42, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSvSmVCySY3W","outputId":"344afa57-0166-4c3d-daa5-2c22bfd27ee8"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import model_selection, naive_bayes, svm\n","from sklearn.metrics import accuracy_score\n","svc = svm.SVC(kernel='rbf', C=1, probability=True).fit(xtrain_ft, ytrain) \n","prediction = svc.predict_proba(xvalid_ft) \n","prediction_int1 = prediction[:,1] >= 0.2 \n","prediction_int1 = prediction_int1.astype(np.int) \n","f1_score(yvalid, prediction_int1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3417085427135678"]},"metadata":{"tags":[]},"execution_count":108}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WbZCFMf-TYR7","outputId":"cb4939d7-73fa-4572-8a95-8fbb1a94da03"},"source":["report1 = classification_report( yvalid, prediction_int1 )\n","print(report1)\n","acc1=accuracy_score(yvalid,prediction_int1)\n","\n","print(\"SVM-RBF(fasttext), Accuracy Score:\",acc1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.73      0.38      0.50       172\n","           1       0.24      0.58      0.34        59\n","\n","    accuracy                           0.43       231\n","   macro avg       0.48      0.48      0.42       231\n","weighted avg       0.60      0.43      0.46       231\n","\n","SVM-RBF(fasttext), Accuracy Score: 0.4329004329004329\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DP6L-jC9Tm-3","outputId":"01375840-8427-4052-e4a5-28527fcd750d"},"source":["from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_ft, ytrain) \n","prediction2 = rf.predict(xvalid_ft)\n","f1_score(yvalid, prediction2)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.17142857142857146"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNbaIyuYTyOw","outputId":"7f67c901-a466-4e4e-fc07-65a99651e4c0"},"source":["report2 = classification_report( yvalid, prediction2 )\n","print(report2)\n","acc2=accuracy_score(yvalid,prediction2)\n","\n","print(\"RF(fasttext), Accuracy Score:\",acc2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.97      0.85       172\n","           1       0.55      0.10      0.17        59\n","\n","    accuracy                           0.75       231\n","   macro avg       0.65      0.54      0.51       231\n","weighted avg       0.70      0.75      0.68       231\n","\n","RF(fasttext), Accuracy Score: 0.7489177489177489\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VKtqRRipUQS2","outputId":"3fa1ed4f-2ca4-49e5-9761-afa52ad8d1ce"},"source":["from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.models import Sequential\n","model = Sequential()\n","model.add(Embedding(500, 120, input_length = xtrain_ft.shape[1]))\n","model.add(SpatialDropout1D(0.4))\n","model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 200, 120)          60000     \n","_________________________________________________________________\n","spatial_dropout1d_3 (Spatial (None, 200, 120)          0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 176)               209088    \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 177       \n","=================================================================\n","Total params: 269,265\n","Trainable params: 269,265\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQDG5uuSUdxI","outputId":"80f33edd-9240-4a5c-fc44-190bf1db9526"},"source":["batch_size=32\n","model.fit(xtrain_ft, ytrain, epochs = 5, batch_size=batch_size, verbose = 'auto')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","29/29 [==============================] - 31s 872ms/step - loss: 0.0000e+00 - accuracy: 0.2690\n","Epoch 2/5\n","29/29 [==============================] - 25s 870ms/step - loss: 0.0000e+00 - accuracy: 0.2501\n","Epoch 3/5\n","29/29 [==============================] - 25s 876ms/step - loss: 0.0000e+00 - accuracy: 0.2380\n","Epoch 4/5\n","29/29 [==============================] - 25s 871ms/step - loss: 0.0000e+00 - accuracy: 0.2693\n","Epoch 5/5\n","29/29 [==============================] - 26s 883ms/step - loss: 0.0000e+00 - accuracy: 0.2482\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fb1bc488f10>"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"code","metadata":{"id":"6NJAdDmtpbH4"},"source":["from __future__ import division\n","import os\n","import numpy \n","import pickle\n","from sklearn import svm\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier \n","from sklearn.model_selection import train_test_split  # Random split into training and test dataset.\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import cross_val_predict\n","#from sklearn.model_selection import cross_validate\n","from sklearn import metrics\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import SVC\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","from sklearn.feature_selection import RFE\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCRc6rDLbo6q"},"source":["gnb = GaussianNB()\n","gnb.fit(xtrain_ft, ytrain)\n","\n","#Predict the response for test dataset\n","y_pred3 = gnb.predict(xvalid_ft)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plirNTpdcOH_","outputId":"c64fcdc6-d962-4163-dea5-4deb8601bc38"},"source":["print(\"Accuracy:\",metrics.accuracy_score(yvalid, y_pred3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.5454545454545454\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z0McKFmdc2Q1"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","knn = KNeighborsClassifier(n_neighbors=5)\n","\n","#Train the model using the training sets\n","knn.fit(xtrain_ft, ytrain)\n","\n","#Predict the response for test dataset\n","y_pred4 = knn.predict(xvalid_ft)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSAKVK9wea3C","outputId":"88f672f3-f89c-46a5-d1a6-22663bec8fc0"},"source":["print(\"Accuracy:\",metrics.accuracy_score(yvalid, y_pred4))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 0.6363636363636364\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Uf-RnpAC_N-"},"source":["Hyper Parameter Tuning"]},{"cell_type":"code","metadata":{"id":"gwI6hhIM3_HN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"951c9fd1-f728-441d-81d8-53294956c1d0"},"source":["svc = svm.SVC()\n","\n","# Create the random grid\n","random_grid = {'C': [1,10,100], \n","              'kernel': ['linear','rbf']}\n","              \n","rf_random = RandomizedSearchCV(estimator = svc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n","# Fit the random search model\n","rf_random.fit(xtrain_ft, ytrain)\n","print(rf_random.best_params_)\n","prediction0 = rf_random.predict(xvalid_ft) \n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction0)\n","print(report)\n","acc0=accuracy_score(yvalid,prediction0)\n","\n","print(\"SVM(fasttext) after HT+RS, Accuracy Score:\",acc0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    3.5s finished\n"],"name":"stderr"},{"output_type":"stream","text":["{'kernel': 'linear', 'C': 1}\n","              precision    recall  f1-score   support\n","\n","           0       0.74      1.00      0.85       172\n","           1       0.00      0.00      0.00        59\n","\n","    accuracy                           0.74       231\n","   macro avg       0.37      0.50      0.43       231\n","weighted avg       0.55      0.74      0.64       231\n","\n","SVM(bow) after HT+RS, Accuracy Score: 0.7445887445887446\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AsqjUI5xMug4"},"source":["#fasttext_df.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1h_k-EQ5gPHq"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score\n","from keras.models import Sequential\n","from keras.layers import Dense, BatchNormalization, Dropout\n","from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from math import floor\n","from sklearn.metrics import make_scorer, accuracy_score\n","from hyperopt import hp, tpe, fmin\n","from sklearn.model_selection import StratifiedKFold\n","from keras.layers import LeakyReLU\n","LeakyReLU = LeakyReLU(alpha=0.1)\n","import warnings\n","warnings.filterwarnings('ignore')\n","pd.set_option(\"display.max_columns\", None)\n","# Make scorer accuracy\n","score_acc = make_scorer(accuracy_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MifdTFE6ztdB","outputId":"95b1a591-6033-41f3-8926-de720e48523e"},"source":["from sklearn.model_selection import GridSearchCV\n","parameters = [{'C': [1,10,100], 'kernel': ['linear','rbf']}]\n","grid_search = GridSearchCV(estimator= svc,\n","                          param_grid = parameters, scoring = 'accuracy',cv = 10)\n","grid_search = grid_search.fit(xtrain_ft, ytrain)\n","print(grid_search.best_params_)\n","prediction = grid_search.predict(xvalid_ft)  \n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction)\n","print(report)\n","acc5=accuracy_score(yvalid,prediction)\n","\n","print(\"SVM(fasttext) after HT+GD, Accuracy Score:\",acc5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'C': 1, 'kernel': 'linear'}\n","              precision    recall  f1-score   support\n","\n","           0       0.74      1.00      0.85       172\n","           1       0.00      0.00      0.00        59\n","\n","    accuracy                           0.74       231\n","   macro avg       0.37      0.50      0.43       231\n","weighted avg       0.55      0.74      0.64       231\n","\n","SVM(word2vec) after HT+GD, Accuracy Score: 0.7445887445887446\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4wf2SVTF0NSQ"},"source":["#svr_random.best_params_, svr_random.best_score_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C1PE4TYuw6CR","outputId":"a2e13057-316f-4f97-f481-80a019b69932"},"source":["from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_ft, ytrain) \n","prediction = rf.predict(xvalid_ft)\n","\n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc6=accuracy_score(yvalid,prediction)\n","\n","print(\"RF, Accuracy Score:\",acc6)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.76      0.97      0.85       172\n","           1       0.55      0.10      0.17        59\n","\n","    accuracy                           0.75       231\n","   macro avg       0.65      0.54      0.51       231\n","weighted avg       0.70      0.75      0.68       231\n","\n","RF(word2vec), Accuracy Score: 0.7489177489177489\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bcwlmn9Myu70","outputId":"9d8daf4a-5126-43cd-9cc3-c9beab9e9b06"},"source":["from sklearn.model_selection import RandomizedSearchCV\n","rf = RandomForestClassifier()\n","#Number of trees in random forest\n","n_estimators = [int(x) for x in np.linspace(start = 500, stop = 1000, num = 100)]\n","# Number of features to consider at every split\n","max_features = ['auto', 'sqrt']\n","# Maximum number of levels in tree\n","max_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\n","max_depth.append(None)\n","# Minimum number of samples required to split a node\n","min_samples_split = [2, 5, 10]\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [1, 2, 4]\n","# Method of selecting samples for training each tree\n","bootstrap = [True]\n","# Create the random grid\n","random_grid = {'n_estimators': n_estimators,\n","               'max_features': max_features,\n","               'max_depth': max_depth,\n","               'min_samples_split': min_samples_split,\n","               'min_samples_leaf': min_samples_leaf,\n","               'bootstrap': bootstrap}\n","rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n","# Fit the random search model\n","rf_random.fit(xtrain_ft, ytrain)\n","\n","print(\"best Params\")\n","print(rf_random.best_params_)\n","\n","prediction = rf_random.predict(xvalid_ft)\n","\n","#classifiction Report\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc7=accuracy_score(yvalid,prediction)\n","\n","print(\"RF(word2vec) after HT+RS, Accuracy Score:\",acc7)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.1min\n","[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  8.4min\n","[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 16.1min finished\n"],"name":"stderr"},{"output_type":"stream","text":["best Params\n","{'n_estimators': 803, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 10, 'bootstrap': True}\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.99      0.85       172\n","           1       0.50      0.03      0.06        59\n","\n","    accuracy                           0.74       231\n","   macro avg       0.62      0.51      0.46       231\n","weighted avg       0.69      0.74      0.65       231\n","\n","RF(word2vec) after HT+RS, Accuracy Score: 0.7445887445887446\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3mNoIcYJNPK","outputId":"dd014ce5-37bb-4433-bbdd-1e2dd768c1f9"},"source":["from sklearn.model_selection import GridSearchCV\n","# Create the parameter grid \n","param_grid = {\n","    'bootstrap': [True],\n","    'max_depth': [80, 90, 100, 110],\n","    'max_features': [2, 3],\n","    'min_samples_leaf': [3, 4, 5],\n","    'min_samples_split': [8, 10, 12],\n","    'n_estimators': [100, 200, 300, 1000]\n","}\n","# Create a based model\n","rf = RandomForestClassifier()\n","# Instantiate the grid search model\n","grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,cv = 3, n_jobs = -1, verbose = 2)\n","# Fit the grid search to the data\n","grid_search.fit(xtrain_ft, ytrain)\n","\n","print(\"best Params\")\n","print(grid_search.best_params_)\n","\n","prediction8 = grid_search.predict(xvalid_ft)\n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc8=accuracy_score(yvalid,prediction)\n","\n","print(\"RF(fasttext) after HT+GS, Accuracy Score:\",acc8)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   25.5s\n","[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.9min\n","[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  4.3min\n","[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  7.8min\n","[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 10.5min finished\n"],"name":"stderr"},{"output_type":"stream","text":["best Params\n","{'bootstrap': True, 'max_depth': 90, 'max_features': 2, 'min_samples_leaf': 3, 'min_samples_split': 12, 'n_estimators': 100}\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.99      0.85       172\n","           1       0.50      0.03      0.06        59\n","\n","    accuracy                           0.74       231\n","   macro avg       0.62      0.51      0.46       231\n","weighted avg       0.69      0.74      0.65       231\n","\n","RF(word2vec) after HT+GS, Accuracy Score: 0.7445887445887446\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KOTHrkeiRrQj"},"source":["Doc2Vec"]},{"cell_type":"code","metadata":{"id":"ljJHloYpRetv"},"source":["from tqdm import tqdm \n","tqdm.pandas(desc=\"progress-bar\") \n","from gensim.models.doc2vec import LabeledSentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxGtiJLrRqZb"},"source":["def add_label(twt):\n","    output = []\n","    for i, s in zip(twt.index, twt):\n","        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n","    return output\n","\n","labeled_tweets = add_label(tokenized_tweet) # label all the tweets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxG0iFE4R7vn","outputId":"39d0d96c-df9a-43a3-c59c-3ff60a362578"},"source":["import gensim\n","model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for â€˜distributed memoryâ€™ model\n","                                  dm_mean=1, # dm_mean = 1 for using mean of the context word vectors\n","                                  vector_size=200, # no. of desired features\n","                                  window=5, # width of the context window                                  \n","                                  negative=7, # if > 0 then negative sampling will be used\n","                                  min_count=5, # Ignores all words with total frequency lower than 5.                                  \n","                                  workers=32, # no. of cores                                  \n","                                  alpha=0.1, # learning rate                                  \n","                                  seed = 23, # for reproducibility\n","                                 ) \n","\n","model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])\n","\n","model_d2v.train(labeled_tweets, total_examples= len(dataset['tweet']), epochs=15)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1153/1153 [00:00<00:00, 1105884.41it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BlEtq1pTTeBd","outputId":"64de91c7-9793-49f8-a692-16f7c10f261b"},"source":["docvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n","for i in range(len(dataset)):\n","    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))    \n","\n","docvec_df = pd.DataFrame(docvec_arrays) \n","docvec_df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1153, 200)"]},"metadata":{"tags":[]},"execution_count":152}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"_1RliITZToJY","outputId":"487960a8-b065-4b1a-ca85-3c83364454fa"},"source":["docvec_df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","      <th>128</th>\n","      <th>129</th>\n","      <th>130</th>\n","      <th>131</th>\n","      <th>132</th>\n","      <th>133</th>\n","      <th>134</th>\n","      <th>135</th>\n","      <th>136</th>\n","      <th>137</th>\n","      <th>138</th>\n","      <th>139</th>\n","      <th>140</th>\n","      <th>141</th>\n","      <th>142</th>\n","      <th>143</th>\n","      <th>144</th>\n","      <th>145</th>\n","      <th>146</th>\n","      <th>147</th>\n","      <th>148</th>\n","      <th>149</th>\n","      <th>150</th>\n","      <th>151</th>\n","      <th>152</th>\n","      <th>153</th>\n","      <th>154</th>\n","      <th>155</th>\n","      <th>156</th>\n","      <th>157</th>\n","      <th>158</th>\n","      <th>159</th>\n","      <th>160</th>\n","      <th>161</th>\n","      <th>162</th>\n","      <th>163</th>\n","      <th>164</th>\n","      <th>165</th>\n","      <th>166</th>\n","      <th>167</th>\n","      <th>168</th>\n","      <th>169</th>\n","      <th>170</th>\n","      <th>171</th>\n","      <th>172</th>\n","      <th>173</th>\n","      <th>174</th>\n","      <th>175</th>\n","      <th>176</th>\n","      <th>177</th>\n","      <th>178</th>\n","      <th>179</th>\n","      <th>180</th>\n","      <th>181</th>\n","      <th>182</th>\n","      <th>183</th>\n","      <th>184</th>\n","      <th>185</th>\n","      <th>186</th>\n","      <th>187</th>\n","      <th>188</th>\n","      <th>189</th>\n","      <th>190</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.055312</td>\n","      <td>-0.039930</td>\n","      <td>0.191448</td>\n","      <td>-0.149319</td>\n","      <td>0.110794</td>\n","      <td>-0.138439</td>\n","      <td>0.051423</td>\n","      <td>-0.003943</td>\n","      <td>-0.007598</td>\n","      <td>-0.065237</td>\n","      <td>0.019513</td>\n","      <td>-0.102994</td>\n","      <td>-0.031106</td>\n","      <td>0.056399</td>\n","      <td>0.085243</td>\n","      <td>-0.045452</td>\n","      <td>-0.049448</td>\n","      <td>-0.090799</td>\n","      <td>0.057514</td>\n","      <td>0.027814</td>\n","      <td>0.194963</td>\n","      <td>-0.144012</td>\n","      <td>0.232128</td>\n","      <td>0.004772</td>\n","      <td>-0.113581</td>\n","      <td>0.028224</td>\n","      <td>-0.103949</td>\n","      <td>0.111155</td>\n","      <td>0.073251</td>\n","      <td>0.021104</td>\n","      <td>-0.152625</td>\n","      <td>0.039723</td>\n","      <td>0.033198</td>\n","      <td>-0.007666</td>\n","      <td>0.077760</td>\n","      <td>-0.008866</td>\n","      <td>-0.102105</td>\n","      <td>-0.035564</td>\n","      <td>0.014161</td>\n","      <td>0.097684</td>\n","      <td>-0.174950</td>\n","      <td>0.006351</td>\n","      <td>0.067718</td>\n","      <td>-0.144566</td>\n","      <td>0.244559</td>\n","      <td>-0.055779</td>\n","      <td>0.055129</td>\n","      <td>-0.174438</td>\n","      <td>0.053647</td>\n","      <td>0.138934</td>\n","      <td>0.128230</td>\n","      <td>-0.037576</td>\n","      <td>-0.095177</td>\n","      <td>0.096715</td>\n","      <td>-0.029224</td>\n","      <td>-0.147459</td>\n","      <td>-0.039733</td>\n","      <td>-0.081511</td>\n","      <td>0.097535</td>\n","      <td>-0.127752</td>\n","      <td>-0.223355</td>\n","      <td>0.064697</td>\n","      <td>-0.067259</td>\n","      <td>0.072545</td>\n","      <td>0.061619</td>\n","      <td>-0.150009</td>\n","      <td>0.152331</td>\n","      <td>-0.194776</td>\n","      <td>-0.020112</td>\n","      <td>-0.021461</td>\n","      <td>0.087508</td>\n","      <td>0.003023</td>\n","      <td>-0.113706</td>\n","      <td>-0.124695</td>\n","      <td>-0.028077</td>\n","      <td>0.069431</td>\n","      <td>-0.201233</td>\n","      <td>-0.066189</td>\n","      <td>0.035115</td>\n","      <td>0.140000</td>\n","      <td>-0.047706</td>\n","      <td>0.146456</td>\n","      <td>-0.048373</td>\n","      <td>-0.056729</td>\n","      <td>0.041842</td>\n","      <td>0.074862</td>\n","      <td>-0.058753</td>\n","      <td>-0.059540</td>\n","      <td>-0.070898</td>\n","      <td>-0.061751</td>\n","      <td>-0.051123</td>\n","      <td>-0.182045</td>\n","      <td>-0.160678</td>\n","      <td>-0.065777</td>\n","      <td>0.074163</td>\n","      <td>-0.073466</td>\n","      <td>-0.267037</td>\n","      <td>-0.158344</td>\n","      <td>0.044062</td>\n","      <td>0.051425</td>\n","      <td>-0.077031</td>\n","      <td>0.114817</td>\n","      <td>-0.182484</td>\n","      <td>-0.070219</td>\n","      <td>0.006282</td>\n","      <td>0.098775</td>\n","      <td>-0.144713</td>\n","      <td>-0.040478</td>\n","      <td>-0.011775</td>\n","      <td>-0.024338</td>\n","      <td>-0.004593</td>\n","      <td>-0.090838</td>\n","      <td>-0.007223</td>\n","      <td>0.002444</td>\n","      <td>-0.090104</td>\n","      <td>0.087139</td>\n","      <td>0.028637</td>\n","      <td>0.114386</td>\n","      <td>-0.059107</td>\n","      <td>0.314040</td>\n","      <td>0.036433</td>\n","      <td>-0.207628</td>\n","      <td>0.121528</td>\n","      <td>0.168863</td>\n","      <td>-0.030016</td>\n","      <td>-0.089476</td>\n","      <td>-0.056296</td>\n","      <td>-0.229768</td>\n","      <td>0.038695</td>\n","      <td>0.229738</td>\n","      <td>0.060928</td>\n","      <td>0.029991</td>\n","      <td>0.077916</td>\n","      <td>0.095506</td>\n","      <td>-0.088583</td>\n","      <td>0.146278</td>\n","      <td>-0.135664</td>\n","      <td>0.075616</td>\n","      <td>-0.050393</td>\n","      <td>0.051478</td>\n","      <td>-0.004462</td>\n","      <td>-0.191311</td>\n","      <td>-0.031969</td>\n","      <td>-0.002654</td>\n","      <td>-0.121763</td>\n","      <td>-0.110812</td>\n","      <td>0.090307</td>\n","      <td>-0.082125</td>\n","      <td>-0.254773</td>\n","      <td>-0.064707</td>\n","      <td>-0.022161</td>\n","      <td>-0.154172</td>\n","      <td>-0.137295</td>\n","      <td>-0.059365</td>\n","      <td>0.067794</td>\n","      <td>-0.069797</td>\n","      <td>-0.033828</td>\n","      <td>-0.001323</td>\n","      <td>0.008541</td>\n","      <td>0.094177</td>\n","      <td>-0.028051</td>\n","      <td>-0.015630</td>\n","      <td>-0.009411</td>\n","      <td>0.041273</td>\n","      <td>0.160046</td>\n","      <td>-0.120427</td>\n","      <td>0.027131</td>\n","      <td>-0.032987</td>\n","      <td>-0.096873</td>\n","      <td>-0.113636</td>\n","      <td>-0.062230</td>\n","      <td>-0.015315</td>\n","      <td>0.050063</td>\n","      <td>0.215760</td>\n","      <td>0.024331</td>\n","      <td>0.083756</td>\n","      <td>0.120729</td>\n","      <td>0.155200</td>\n","      <td>0.004358</td>\n","      <td>0.013040</td>\n","      <td>-0.203371</td>\n","      <td>0.086246</td>\n","      <td>0.128653</td>\n","      <td>0.075776</td>\n","      <td>-0.005553</td>\n","      <td>-0.010128</td>\n","      <td>-0.016698</td>\n","      <td>-0.009678</td>\n","      <td>-0.069997</td>\n","      <td>0.136486</td>\n","      <td>0.026843</td>\n","      <td>-0.032314</td>\n","      <td>0.014119</td>\n","      <td>-0.067477</td>\n","      <td>-0.133395</td>\n","      <td>-0.078424</td>\n","      <td>0.032957</td>\n","      <td>0.046098</td>\n","      <td>0.003512</td>\n","      <td>-0.021471</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.061018</td>\n","      <td>0.016988</td>\n","      <td>0.049502</td>\n","      <td>-0.042556</td>\n","      <td>0.012166</td>\n","      <td>0.042974</td>\n","      <td>0.143421</td>\n","      <td>-0.050513</td>\n","      <td>-0.014999</td>\n","      <td>-0.021124</td>\n","      <td>-0.103508</td>\n","      <td>-0.014746</td>\n","      <td>-0.062674</td>\n","      <td>0.039278</td>\n","      <td>-0.023490</td>\n","      <td>-0.005170</td>\n","      <td>-0.032101</td>\n","      <td>-0.067672</td>\n","      <td>-0.047502</td>\n","      <td>0.086716</td>\n","      <td>0.029468</td>\n","      <td>-0.049504</td>\n","      <td>0.094705</td>\n","      <td>0.053743</td>\n","      <td>-0.040958</td>\n","      <td>-0.005397</td>\n","      <td>-0.128751</td>\n","      <td>0.031853</td>\n","      <td>0.037736</td>\n","      <td>0.099800</td>\n","      <td>-0.058445</td>\n","      <td>-0.102526</td>\n","      <td>0.049032</td>\n","      <td>-0.041298</td>\n","      <td>0.129646</td>\n","      <td>0.103363</td>\n","      <td>-0.021137</td>\n","      <td>-0.025913</td>\n","      <td>0.037137</td>\n","      <td>-0.038500</td>\n","      <td>-0.128343</td>\n","      <td>-0.006980</td>\n","      <td>0.078419</td>\n","      <td>0.011923</td>\n","      <td>0.009650</td>\n","      <td>-0.093427</td>\n","      <td>0.108384</td>\n","      <td>0.033260</td>\n","      <td>0.045413</td>\n","      <td>-0.094165</td>\n","      <td>-0.043469</td>\n","      <td>-0.053775</td>\n","      <td>0.057963</td>\n","      <td>0.072712</td>\n","      <td>0.043468</td>\n","      <td>-0.044517</td>\n","      <td>0.075127</td>\n","      <td>-0.026359</td>\n","      <td>0.006359</td>\n","      <td>0.043224</td>\n","      <td>-0.153530</td>\n","      <td>0.080685</td>\n","      <td>-0.070069</td>\n","      <td>0.052864</td>\n","      <td>0.091646</td>\n","      <td>-0.095059</td>\n","      <td>-0.055839</td>\n","      <td>-0.043282</td>\n","      <td>-0.063824</td>\n","      <td>0.066563</td>\n","      <td>0.056411</td>\n","      <td>-0.015844</td>\n","      <td>-0.008193</td>\n","      <td>-0.215822</td>\n","      <td>0.040523</td>\n","      <td>-0.049753</td>\n","      <td>-0.111441</td>\n","      <td>-0.091611</td>\n","      <td>-0.029417</td>\n","      <td>0.086315</td>\n","      <td>0.032174</td>\n","      <td>-0.005204</td>\n","      <td>0.040144</td>\n","      <td>-0.071306</td>\n","      <td>-0.071806</td>\n","      <td>-0.002281</td>\n","      <td>0.022584</td>\n","      <td>0.052517</td>\n","      <td>0.117852</td>\n","      <td>-0.008148</td>\n","      <td>-0.030722</td>\n","      <td>-0.027184</td>\n","      <td>-0.027622</td>\n","      <td>0.034868</td>\n","      <td>0.004980</td>\n","      <td>-0.042590</td>\n","      <td>-0.114096</td>\n","      <td>0.058591</td>\n","      <td>0.055083</td>\n","      <td>-0.052130</td>\n","      <td>-0.016828</td>\n","      <td>-0.018380</td>\n","      <td>-0.044166</td>\n","      <td>-0.114783</td>\n","      <td>0.082226</td>\n","      <td>0.158211</td>\n","      <td>-0.062394</td>\n","      <td>0.001910</td>\n","      <td>-0.037969</td>\n","      <td>-0.100493</td>\n","      <td>-0.035638</td>\n","      <td>-0.011532</td>\n","      <td>0.055595</td>\n","      <td>0.023990</td>\n","      <td>0.119399</td>\n","      <td>-0.001851</td>\n","      <td>0.071994</td>\n","      <td>-0.086471</td>\n","      <td>0.092134</td>\n","      <td>-0.022091</td>\n","      <td>-0.041771</td>\n","      <td>-0.097996</td>\n","      <td>-0.017188</td>\n","      <td>0.001062</td>\n","      <td>0.062520</td>\n","      <td>-0.069970</td>\n","      <td>-0.066527</td>\n","      <td>-0.044718</td>\n","      <td>0.074902</td>\n","      <td>-0.002593</td>\n","      <td>0.096486</td>\n","      <td>-0.111685</td>\n","      <td>-0.041615</td>\n","      <td>0.026480</td>\n","      <td>-0.017918</td>\n","      <td>0.017210</td>\n","      <td>-0.164579</td>\n","      <td>0.100271</td>\n","      <td>-0.208402</td>\n","      <td>0.032018</td>\n","      <td>0.023155</td>\n","      <td>0.037884</td>\n","      <td>-0.026814</td>\n","      <td>0.044153</td>\n","      <td>-0.153497</td>\n","      <td>-0.021794</td>\n","      <td>-0.028157</td>\n","      <td>-0.038355</td>\n","      <td>-0.059844</td>\n","      <td>-0.019138</td>\n","      <td>-0.086807</td>\n","      <td>0.033628</td>\n","      <td>-0.007282</td>\n","      <td>-0.051670</td>\n","      <td>0.044085</td>\n","      <td>-0.115107</td>\n","      <td>0.009230</td>\n","      <td>0.029948</td>\n","      <td>0.043391</td>\n","      <td>0.081878</td>\n","      <td>0.021124</td>\n","      <td>0.009889</td>\n","      <td>0.062498</td>\n","      <td>0.015040</td>\n","      <td>0.014968</td>\n","      <td>0.013056</td>\n","      <td>-0.045615</td>\n","      <td>-0.188553</td>\n","      <td>-0.074055</td>\n","      <td>-0.081047</td>\n","      <td>0.007862</td>\n","      <td>-0.012802</td>\n","      <td>0.060702</td>\n","      <td>0.092859</td>\n","      <td>0.011237</td>\n","      <td>0.004092</td>\n","      <td>-0.036368</td>\n","      <td>0.031882</td>\n","      <td>0.022227</td>\n","      <td>0.021023</td>\n","      <td>-0.102012</td>\n","      <td>-0.057878</td>\n","      <td>0.048686</td>\n","      <td>0.085081</td>\n","      <td>-0.030369</td>\n","      <td>0.025377</td>\n","      <td>0.032599</td>\n","      <td>-0.000141</td>\n","      <td>-0.025085</td>\n","      <td>0.017068</td>\n","      <td>0.017619</td>\n","      <td>0.026190</td>\n","      <td>-0.089510</td>\n","      <td>-0.028861</td>\n","      <td>0.039401</td>\n","      <td>0.096074</td>\n","      <td>-0.070603</td>\n","      <td>0.047572</td>\n","      <td>-0.006124</td>\n","      <td>-0.083557</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.003201</td>\n","      <td>0.049834</td>\n","      <td>0.065356</td>\n","      <td>-0.082197</td>\n","      <td>0.084950</td>\n","      <td>-0.005540</td>\n","      <td>0.161819</td>\n","      <td>-0.071908</td>\n","      <td>-0.037254</td>\n","      <td>-0.009837</td>\n","      <td>-0.063657</td>\n","      <td>0.012028</td>\n","      <td>-0.054765</td>\n","      <td>0.032465</td>\n","      <td>0.004423</td>\n","      <td>-0.055838</td>\n","      <td>-0.060516</td>\n","      <td>-0.075242</td>\n","      <td>-0.031401</td>\n","      <td>0.052626</td>\n","      <td>0.052314</td>\n","      <td>-0.049840</td>\n","      <td>0.120136</td>\n","      <td>0.079043</td>\n","      <td>-0.048444</td>\n","      <td>-0.025812</td>\n","      <td>-0.128978</td>\n","      <td>0.043248</td>\n","      <td>0.032008</td>\n","      <td>0.110779</td>\n","      <td>-0.100398</td>\n","      <td>-0.016027</td>\n","      <td>0.030733</td>\n","      <td>-0.025223</td>\n","      <td>0.096444</td>\n","      <td>0.093381</td>\n","      <td>-0.024996</td>\n","      <td>-0.037410</td>\n","      <td>0.049682</td>\n","      <td>0.014305</td>\n","      <td>-0.106185</td>\n","      <td>-0.029681</td>\n","      <td>0.040773</td>\n","      <td>-0.003502</td>\n","      <td>0.059676</td>\n","      <td>-0.082905</td>\n","      <td>0.118139</td>\n","      <td>0.008984</td>\n","      <td>0.080412</td>\n","      <td>-0.006643</td>\n","      <td>-0.014248</td>\n","      <td>-0.062612</td>\n","      <td>0.001453</td>\n","      <td>0.065533</td>\n","      <td>0.036103</td>\n","      <td>-0.093621</td>\n","      <td>0.044886</td>\n","      <td>-0.023221</td>\n","      <td>-0.040951</td>\n","      <td>0.020021</td>\n","      <td>-0.070897</td>\n","      <td>0.058370</td>\n","      <td>-0.095237</td>\n","      <td>-0.006513</td>\n","      <td>0.072156</td>\n","      <td>-0.061076</td>\n","      <td>0.004190</td>\n","      <td>-0.082927</td>\n","      <td>-0.100370</td>\n","      <td>0.002773</td>\n","      <td>0.075639</td>\n","      <td>0.028037</td>\n","      <td>-0.016752</td>\n","      <td>-0.231114</td>\n","      <td>-0.002973</td>\n","      <td>-0.057661</td>\n","      <td>-0.078610</td>\n","      <td>-0.091225</td>\n","      <td>-0.000109</td>\n","      <td>0.132210</td>\n","      <td>0.059829</td>\n","      <td>0.031464</td>\n","      <td>0.046621</td>\n","      <td>-0.082754</td>\n","      <td>-0.034180</td>\n","      <td>0.047502</td>\n","      <td>0.002053</td>\n","      <td>0.009119</td>\n","      <td>0.010232</td>\n","      <td>-0.003966</td>\n","      <td>-0.050242</td>\n","      <td>-0.056497</td>\n","      <td>-0.046894</td>\n","      <td>-0.024984</td>\n","      <td>0.019060</td>\n","      <td>-0.027742</td>\n","      <td>-0.112245</td>\n","      <td>-0.023793</td>\n","      <td>0.006148</td>\n","      <td>0.025831</td>\n","      <td>-0.026951</td>\n","      <td>-0.032982</td>\n","      <td>-0.043011</td>\n","      <td>-0.087830</td>\n","      <td>0.060428</td>\n","      <td>0.132168</td>\n","      <td>-0.018053</td>\n","      <td>-0.048712</td>\n","      <td>-0.017970</td>\n","      <td>-0.008626</td>\n","      <td>-0.007273</td>\n","      <td>-0.005247</td>\n","      <td>0.023876</td>\n","      <td>-0.010678</td>\n","      <td>0.069116</td>\n","      <td>0.035054</td>\n","      <td>0.050647</td>\n","      <td>-0.039136</td>\n","      <td>0.019373</td>\n","      <td>0.026349</td>\n","      <td>0.011268</td>\n","      <td>-0.048878</td>\n","      <td>0.026545</td>\n","      <td>-0.018460</td>\n","      <td>0.089342</td>\n","      <td>-0.034219</td>\n","      <td>-0.034568</td>\n","      <td>-0.053844</td>\n","      <td>0.011709</td>\n","      <td>0.069690</td>\n","      <td>0.127932</td>\n","      <td>-0.058237</td>\n","      <td>-0.069246</td>\n","      <td>0.026614</td>\n","      <td>-0.012108</td>\n","      <td>0.066991</td>\n","      <td>-0.121714</td>\n","      <td>0.061513</td>\n","      <td>-0.115687</td>\n","      <td>0.053551</td>\n","      <td>0.002006</td>\n","      <td>-0.028892</td>\n","      <td>-0.054182</td>\n","      <td>0.040983</td>\n","      <td>-0.158888</td>\n","      <td>-0.035364</td>\n","      <td>0.004313</td>\n","      <td>-0.097934</td>\n","      <td>-0.111168</td>\n","      <td>-0.054865</td>\n","      <td>-0.039240</td>\n","      <td>0.059383</td>\n","      <td>-0.037972</td>\n","      <td>-0.051662</td>\n","      <td>0.058816</td>\n","      <td>-0.136445</td>\n","      <td>0.052160</td>\n","      <td>0.056786</td>\n","      <td>0.047382</td>\n","      <td>0.039975</td>\n","      <td>-0.009758</td>\n","      <td>0.000636</td>\n","      <td>0.062056</td>\n","      <td>0.016325</td>\n","      <td>0.084557</td>\n","      <td>-0.015950</td>\n","      <td>-0.018937</td>\n","      <td>-0.135114</td>\n","      <td>-0.052426</td>\n","      <td>-0.051657</td>\n","      <td>-0.007431</td>\n","      <td>0.016893</td>\n","      <td>0.063027</td>\n","      <td>0.097134</td>\n","      <td>0.007296</td>\n","      <td>0.040619</td>\n","      <td>-0.063901</td>\n","      <td>0.019059</td>\n","      <td>0.003622</td>\n","      <td>-0.008092</td>\n","      <td>-0.108846</td>\n","      <td>0.005799</td>\n","      <td>0.059813</td>\n","      <td>0.059305</td>\n","      <td>-0.040137</td>\n","      <td>-0.002679</td>\n","      <td>-0.027258</td>\n","      <td>0.066232</td>\n","      <td>-0.055880</td>\n","      <td>0.029591</td>\n","      <td>0.043968</td>\n","      <td>0.024549</td>\n","      <td>-0.044297</td>\n","      <td>-0.019310</td>\n","      <td>-0.005018</td>\n","      <td>0.034421</td>\n","      <td>-0.040792</td>\n","      <td>0.029748</td>\n","      <td>0.004514</td>\n","      <td>-0.069791</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.029223</td>\n","      <td>0.033038</td>\n","      <td>0.016233</td>\n","      <td>-0.010071</td>\n","      <td>-0.024444</td>\n","      <td>-0.029503</td>\n","      <td>0.071805</td>\n","      <td>-0.021355</td>\n","      <td>-0.013895</td>\n","      <td>0.017048</td>\n","      <td>-0.013460</td>\n","      <td>-0.023246</td>\n","      <td>-0.118383</td>\n","      <td>0.059493</td>\n","      <td>0.034331</td>\n","      <td>0.009509</td>\n","      <td>-0.087391</td>\n","      <td>-0.008946</td>\n","      <td>-0.027795</td>\n","      <td>0.109961</td>\n","      <td>-0.019577</td>\n","      <td>-0.000089</td>\n","      <td>-0.002808</td>\n","      <td>0.040342</td>\n","      <td>-0.063464</td>\n","      <td>-0.025714</td>\n","      <td>-0.052038</td>\n","      <td>0.011922</td>\n","      <td>-0.017488</td>\n","      <td>0.124574</td>\n","      <td>-0.083677</td>\n","      <td>-0.114848</td>\n","      <td>0.070005</td>\n","      <td>0.001270</td>\n","      <td>0.132067</td>\n","      <td>0.039418</td>\n","      <td>-0.025030</td>\n","      <td>-0.047605</td>\n","      <td>-0.019201</td>\n","      <td>-0.056250</td>\n","      <td>-0.076681</td>\n","      <td>0.052190</td>\n","      <td>0.003089</td>\n","      <td>-0.014640</td>\n","      <td>0.045362</td>\n","      <td>-0.131532</td>\n","      <td>0.102838</td>\n","      <td>0.024087</td>\n","      <td>0.091748</td>\n","      <td>-0.113966</td>\n","      <td>-0.054380</td>\n","      <td>-0.008877</td>\n","      <td>-0.023470</td>\n","      <td>0.016027</td>\n","      <td>0.072124</td>\n","      <td>-0.068161</td>\n","      <td>-0.004273</td>\n","      <td>-0.014254</td>\n","      <td>-0.047378</td>\n","      <td>0.041782</td>\n","      <td>-0.113214</td>\n","      <td>0.022322</td>\n","      <td>-0.037948</td>\n","      <td>0.081923</td>\n","      <td>0.126138</td>\n","      <td>-0.047886</td>\n","      <td>-0.095410</td>\n","      <td>-0.044869</td>\n","      <td>-0.126054</td>\n","      <td>0.058415</td>\n","      <td>0.054089</td>\n","      <td>0.064041</td>\n","      <td>-0.000390</td>\n","      <td>-0.169077</td>\n","      <td>0.069603</td>\n","      <td>-0.046155</td>\n","      <td>-0.017805</td>\n","      <td>-0.039797</td>\n","      <td>0.001620</td>\n","      <td>0.113422</td>\n","      <td>0.002681</td>\n","      <td>-0.005993</td>\n","      <td>0.016260</td>\n","      <td>-0.000945</td>\n","      <td>-0.043262</td>\n","      <td>-0.041749</td>\n","      <td>0.011798</td>\n","      <td>0.030224</td>\n","      <td>0.045672</td>\n","      <td>0.039913</td>\n","      <td>-0.021678</td>\n","      <td>-0.093443</td>\n","      <td>0.020153</td>\n","      <td>-0.041662</td>\n","      <td>0.080349</td>\n","      <td>-0.014209</td>\n","      <td>-0.014459</td>\n","      <td>0.005705</td>\n","      <td>-0.016403</td>\n","      <td>0.001628</td>\n","      <td>-0.029381</td>\n","      <td>-0.045193</td>\n","      <td>-0.032422</td>\n","      <td>-0.056614</td>\n","      <td>0.023511</td>\n","      <td>0.141625</td>\n","      <td>-0.021386</td>\n","      <td>0.021874</td>\n","      <td>-0.026763</td>\n","      <td>0.005500</td>\n","      <td>-0.024027</td>\n","      <td>0.084337</td>\n","      <td>0.053580</td>\n","      <td>-0.016007</td>\n","      <td>0.078336</td>\n","      <td>0.039094</td>\n","      <td>0.070058</td>\n","      <td>-0.123502</td>\n","      <td>0.026837</td>\n","      <td>-0.048717</td>\n","      <td>-0.017742</td>\n","      <td>-0.109955</td>\n","      <td>-0.013687</td>\n","      <td>0.064595</td>\n","      <td>0.011309</td>\n","      <td>-0.006182</td>\n","      <td>-0.093979</td>\n","      <td>0.006954</td>\n","      <td>0.026299</td>\n","      <td>-0.047437</td>\n","      <td>0.064258</td>\n","      <td>-0.076026</td>\n","      <td>-0.065328</td>\n","      <td>-0.035491</td>\n","      <td>-0.034816</td>\n","      <td>-0.004179</td>\n","      <td>-0.081492</td>\n","      <td>0.088453</td>\n","      <td>-0.194769</td>\n","      <td>0.118167</td>\n","      <td>0.013936</td>\n","      <td>0.040369</td>\n","      <td>-0.035155</td>\n","      <td>0.048860</td>\n","      <td>-0.050524</td>\n","      <td>0.022171</td>\n","      <td>-0.033376</td>\n","      <td>-0.109460</td>\n","      <td>-0.088010</td>\n","      <td>-0.008427</td>\n","      <td>-0.107055</td>\n","      <td>-0.011149</td>\n","      <td>-0.099718</td>\n","      <td>-0.065303</td>\n","      <td>-0.013957</td>\n","      <td>-0.089034</td>\n","      <td>0.107907</td>\n","      <td>-0.039467</td>\n","      <td>0.037774</td>\n","      <td>0.082743</td>\n","      <td>0.027358</td>\n","      <td>0.012457</td>\n","      <td>0.048642</td>\n","      <td>0.124568</td>\n","      <td>0.000198</td>\n","      <td>0.057655</td>\n","      <td>-0.040044</td>\n","      <td>-0.209324</td>\n","      <td>-0.028382</td>\n","      <td>-0.000596</td>\n","      <td>-0.014994</td>\n","      <td>0.044176</td>\n","      <td>0.119730</td>\n","      <td>0.023371</td>\n","      <td>0.008801</td>\n","      <td>-0.044613</td>\n","      <td>-0.027303</td>\n","      <td>0.005224</td>\n","      <td>0.005985</td>\n","      <td>0.081936</td>\n","      <td>-0.051553</td>\n","      <td>-0.136268</td>\n","      <td>-0.014287</td>\n","      <td>0.077905</td>\n","      <td>-0.045905</td>\n","      <td>-0.051808</td>\n","      <td>-0.004229</td>\n","      <td>-0.054282</td>\n","      <td>0.021064</td>\n","      <td>-0.064778</td>\n","      <td>-0.023319</td>\n","      <td>-0.011133</td>\n","      <td>-0.087044</td>\n","      <td>-0.022872</td>\n","      <td>0.089331</td>\n","      <td>0.092386</td>\n","      <td>-0.026935</td>\n","      <td>0.097230</td>\n","      <td>-0.034497</td>\n","      <td>-0.073435</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.043262</td>\n","      <td>0.043893</td>\n","      <td>0.056917</td>\n","      <td>-0.098647</td>\n","      <td>0.092812</td>\n","      <td>-0.017589</td>\n","      <td>0.078938</td>\n","      <td>-0.052581</td>\n","      <td>0.008699</td>\n","      <td>-0.016099</td>\n","      <td>-0.023499</td>\n","      <td>0.021725</td>\n","      <td>0.067992</td>\n","      <td>-0.026848</td>\n","      <td>0.022914</td>\n","      <td>-0.072122</td>\n","      <td>-0.065780</td>\n","      <td>-0.069934</td>\n","      <td>0.019467</td>\n","      <td>0.046615</td>\n","      <td>0.036055</td>\n","      <td>-0.052760</td>\n","      <td>0.086948</td>\n","      <td>0.067870</td>\n","      <td>-0.012917</td>\n","      <td>-0.021081</td>\n","      <td>-0.091686</td>\n","      <td>0.005725</td>\n","      <td>-0.007029</td>\n","      <td>0.068535</td>\n","      <td>-0.032816</td>\n","      <td>0.046015</td>\n","      <td>0.058832</td>\n","      <td>0.015537</td>\n","      <td>-0.004533</td>\n","      <td>0.016547</td>\n","      <td>-0.073101</td>\n","      <td>-0.088147</td>\n","      <td>0.039370</td>\n","      <td>0.048142</td>\n","      <td>-0.019322</td>\n","      <td>-0.097676</td>\n","      <td>0.041618</td>\n","      <td>-0.011262</td>\n","      <td>0.069167</td>\n","      <td>-0.044980</td>\n","      <td>0.106995</td>\n","      <td>-0.024451</td>\n","      <td>-0.006377</td>\n","      <td>0.035807</td>\n","      <td>0.022758</td>\n","      <td>-0.071449</td>\n","      <td>-0.040083</td>\n","      <td>0.037616</td>\n","      <td>-0.036219</td>\n","      <td>-0.097986</td>\n","      <td>-0.035913</td>\n","      <td>-0.033559</td>\n","      <td>-0.045972</td>\n","      <td>-0.006644</td>\n","      <td>-0.003945</td>\n","      <td>0.045789</td>\n","      <td>-0.043402</td>\n","      <td>-0.048720</td>\n","      <td>0.029097</td>\n","      <td>-0.006938</td>\n","      <td>0.084531</td>\n","      <td>-0.043571</td>\n","      <td>-0.023949</td>\n","      <td>-0.084357</td>\n","      <td>0.009316</td>\n","      <td>0.004477</td>\n","      <td>0.020502</td>\n","      <td>-0.118127</td>\n","      <td>-0.043620</td>\n","      <td>-0.027257</td>\n","      <td>-0.038873</td>\n","      <td>-0.044611</td>\n","      <td>0.030928</td>\n","      <td>0.083007</td>\n","      <td>0.064508</td>\n","      <td>0.046536</td>\n","      <td>0.066158</td>\n","      <td>-0.057566</td>\n","      <td>-0.014875</td>\n","      <td>0.081208</td>\n","      <td>0.009624</td>\n","      <td>0.023848</td>\n","      <td>-0.077927</td>\n","      <td>-0.024666</td>\n","      <td>-0.066208</td>\n","      <td>-0.070249</td>\n","      <td>-0.054624</td>\n","      <td>-0.032657</td>\n","      <td>0.033347</td>\n","      <td>-0.009000</td>\n","      <td>-0.085000</td>\n","      <td>-0.052862</td>\n","      <td>-0.024535</td>\n","      <td>0.025881</td>\n","      <td>-0.052433</td>\n","      <td>0.012753</td>\n","      <td>-0.011142</td>\n","      <td>-0.022926</td>\n","      <td>-0.029888</td>\n","      <td>0.021118</td>\n","      <td>0.071407</td>\n","      <td>-0.083502</td>\n","      <td>-0.013588</td>\n","      <td>0.082711</td>\n","      <td>0.057966</td>\n","      <td>-0.056847</td>\n","      <td>0.018287</td>\n","      <td>-0.001665</td>\n","      <td>-0.024429</td>\n","      <td>0.032644</td>\n","      <td>-0.043036</td>\n","      <td>0.024678</td>\n","      <td>-0.045200</td>\n","      <td>0.063647</td>\n","      <td>0.049025</td>\n","      <td>0.019913</td>\n","      <td>0.066489</td>\n","      <td>-0.025849</td>\n","      <td>0.028416</td>\n","      <td>0.010319</td>\n","      <td>0.002876</td>\n","      <td>-0.085854</td>\n","      <td>-0.083701</td>\n","      <td>0.077935</td>\n","      <td>0.054166</td>\n","      <td>0.018909</td>\n","      <td>-0.019035</td>\n","      <td>0.080205</td>\n","      <td>-0.016994</td>\n","      <td>0.112154</td>\n","      <td>-0.012276</td>\n","      <td>-0.002677</td>\n","      <td>0.033562</td>\n","      <td>0.007683</td>\n","      <td>-0.062404</td>\n","      <td>-0.034798</td>\n","      <td>-0.015451</td>\n","      <td>0.019375</td>\n","      <td>-0.116244</td>\n","      <td>-0.031569</td>\n","      <td>0.049624</td>\n","      <td>-0.021415</td>\n","      <td>-0.088397</td>\n","      <td>-0.059330</td>\n","      <td>0.037670</td>\n","      <td>0.020383</td>\n","      <td>-0.017984</td>\n","      <td>-0.025215</td>\n","      <td>0.012125</td>\n","      <td>-0.093454</td>\n","      <td>0.035094</td>\n","      <td>0.058969</td>\n","      <td>-0.014608</td>\n","      <td>-0.058347</td>\n","      <td>0.010617</td>\n","      <td>-0.016558</td>\n","      <td>0.039073</td>\n","      <td>-0.066809</td>\n","      <td>0.052841</td>\n","      <td>-0.028467</td>\n","      <td>0.019787</td>\n","      <td>0.000242</td>\n","      <td>-0.038667</td>\n","      <td>-0.037864</td>\n","      <td>-0.003309</td>\n","      <td>0.004975</td>\n","      <td>-0.003108</td>\n","      <td>0.107157</td>\n","      <td>-0.033303</td>\n","      <td>0.084484</td>\n","      <td>-0.039878</td>\n","      <td>-0.053463</td>\n","      <td>-0.008048</td>\n","      <td>-0.085443</td>\n","      <td>-0.037588</td>\n","      <td>0.043686</td>\n","      <td>0.073742</td>\n","      <td>0.012074</td>\n","      <td>-0.027139</td>\n","      <td>-0.033541</td>\n","      <td>-0.077054</td>\n","      <td>0.087032</td>\n","      <td>-0.057262</td>\n","      <td>0.041101</td>\n","      <td>0.030200</td>\n","      <td>-0.024849</td>\n","      <td>0.010870</td>\n","      <td>-0.008267</td>\n","      <td>0.004219</td>\n","      <td>-0.031360</td>\n","      <td>0.053717</td>\n","      <td>0.022858</td>\n","      <td>0.001484</td>\n","      <td>-0.025883</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1148</th>\n","      <td>-0.025763</td>\n","      <td>0.071045</td>\n","      <td>0.037128</td>\n","      <td>-0.129943</td>\n","      <td>0.093425</td>\n","      <td>-0.004798</td>\n","      <td>0.139285</td>\n","      <td>-0.073269</td>\n","      <td>0.022682</td>\n","      <td>0.003862</td>\n","      <td>-0.024284</td>\n","      <td>0.065725</td>\n","      <td>0.033895</td>\n","      <td>0.024953</td>\n","      <td>0.002252</td>\n","      <td>0.014924</td>\n","      <td>-0.017935</td>\n","      <td>-0.072112</td>\n","      <td>-0.028729</td>\n","      <td>0.066526</td>\n","      <td>0.037607</td>\n","      <td>-0.099614</td>\n","      <td>0.171433</td>\n","      <td>-0.030529</td>\n","      <td>0.008454</td>\n","      <td>0.002131</td>\n","      <td>-0.093959</td>\n","      <td>0.073591</td>\n","      <td>0.117216</td>\n","      <td>0.081128</td>\n","      <td>-0.061986</td>\n","      <td>-0.011908</td>\n","      <td>0.097863</td>\n","      <td>0.072944</td>\n","      <td>0.090765</td>\n","      <td>0.187357</td>\n","      <td>-0.140266</td>\n","      <td>-0.026925</td>\n","      <td>0.030918</td>\n","      <td>-0.071531</td>\n","      <td>-0.056693</td>\n","      <td>0.057678</td>\n","      <td>0.033244</td>\n","      <td>0.058875</td>\n","      <td>0.034093</td>\n","      <td>-0.071130</td>\n","      <td>0.182654</td>\n","      <td>0.057474</td>\n","      <td>-0.013123</td>\n","      <td>-0.006828</td>\n","      <td>-0.027016</td>\n","      <td>-0.069627</td>\n","      <td>0.038774</td>\n","      <td>0.038216</td>\n","      <td>0.051217</td>\n","      <td>-0.062813</td>\n","      <td>0.126596</td>\n","      <td>0.002623</td>\n","      <td>0.023030</td>\n","      <td>0.071060</td>\n","      <td>0.021068</td>\n","      <td>0.072009</td>\n","      <td>0.056083</td>\n","      <td>0.000626</td>\n","      <td>0.064103</td>\n","      <td>-0.120418</td>\n","      <td>-0.032645</td>\n","      <td>-0.031961</td>\n","      <td>-0.071685</td>\n","      <td>0.000026</td>\n","      <td>0.123615</td>\n","      <td>-0.044415</td>\n","      <td>-0.015697</td>\n","      <td>-0.246760</td>\n","      <td>0.011643</td>\n","      <td>-0.094024</td>\n","      <td>-0.129853</td>\n","      <td>-0.125414</td>\n","      <td>-0.093487</td>\n","      <td>0.190303</td>\n","      <td>0.075941</td>\n","      <td>-0.030443</td>\n","      <td>-0.016166</td>\n","      <td>-0.068983</td>\n","      <td>-0.046988</td>\n","      <td>-0.014944</td>\n","      <td>-0.005712</td>\n","      <td>0.049873</td>\n","      <td>0.090093</td>\n","      <td>-0.106615</td>\n","      <td>-0.028315</td>\n","      <td>0.026453</td>\n","      <td>-0.100344</td>\n","      <td>0.050182</td>\n","      <td>-0.057447</td>\n","      <td>-0.077865</td>\n","      <td>-0.128419</td>\n","      <td>-0.034301</td>\n","      <td>-0.052331</td>\n","      <td>0.061794</td>\n","      <td>0.003536</td>\n","      <td>-0.071607</td>\n","      <td>-0.048121</td>\n","      <td>-0.089022</td>\n","      <td>-0.006083</td>\n","      <td>0.088820</td>\n","      <td>-0.022778</td>\n","      <td>-0.051379</td>\n","      <td>-0.058044</td>\n","      <td>-0.063221</td>\n","      <td>-0.001680</td>\n","      <td>-0.028314</td>\n","      <td>0.003729</td>\n","      <td>-0.033152</td>\n","      <td>0.089003</td>\n","      <td>0.041804</td>\n","      <td>0.021812</td>\n","      <td>0.056624</td>\n","      <td>0.030852</td>\n","      <td>-0.023005</td>\n","      <td>-0.112521</td>\n","      <td>-0.052762</td>\n","      <td>0.082858</td>\n","      <td>0.018961</td>\n","      <td>0.065693</td>\n","      <td>-0.035522</td>\n","      <td>-0.018624</td>\n","      <td>-0.027916</td>\n","      <td>0.056331</td>\n","      <td>0.139032</td>\n","      <td>0.067437</td>\n","      <td>-0.075481</td>\n","      <td>0.006476</td>\n","      <td>0.021048</td>\n","      <td>-0.045508</td>\n","      <td>0.095951</td>\n","      <td>-0.122612</td>\n","      <td>0.085910</td>\n","      <td>-0.046145</td>\n","      <td>0.042162</td>\n","      <td>-0.042910</td>\n","      <td>-0.049483</td>\n","      <td>0.044537</td>\n","      <td>-0.016423</td>\n","      <td>-0.187987</td>\n","      <td>0.004531</td>\n","      <td>0.010061</td>\n","      <td>-0.069784</td>\n","      <td>0.085104</td>\n","      <td>-0.072495</td>\n","      <td>-0.049618</td>\n","      <td>0.100725</td>\n","      <td>0.048605</td>\n","      <td>-0.096432</td>\n","      <td>0.065582</td>\n","      <td>-0.120835</td>\n","      <td>0.127561</td>\n","      <td>0.037443</td>\n","      <td>-0.000482</td>\n","      <td>0.020684</td>\n","      <td>-0.021025</td>\n","      <td>-0.035419</td>\n","      <td>0.003044</td>\n","      <td>-0.025148</td>\n","      <td>-0.011260</td>\n","      <td>-0.070878</td>\n","      <td>-0.085629</td>\n","      <td>-0.121183</td>\n","      <td>-0.051895</td>\n","      <td>-0.078092</td>\n","      <td>-0.004124</td>\n","      <td>-0.027498</td>\n","      <td>0.124604</td>\n","      <td>0.056136</td>\n","      <td>0.055589</td>\n","      <td>0.062818</td>\n","      <td>-0.076826</td>\n","      <td>0.014304</td>\n","      <td>-0.072733</td>\n","      <td>-0.111488</td>\n","      <td>-0.018519</td>\n","      <td>0.071011</td>\n","      <td>0.032206</td>\n","      <td>0.107525</td>\n","      <td>-0.071622</td>\n","      <td>0.044711</td>\n","      <td>0.043170</td>\n","      <td>0.086394</td>\n","      <td>0.022205</td>\n","      <td>-0.013824</td>\n","      <td>0.003970</td>\n","      <td>0.058073</td>\n","      <td>-0.036994</td>\n","      <td>-0.010182</td>\n","      <td>-0.019982</td>\n","      <td>0.016088</td>\n","      <td>-0.021323</td>\n","      <td>0.099319</td>\n","      <td>-0.008026</td>\n","      <td>-0.114344</td>\n","    </tr>\n","    <tr>\n","      <th>1149</th>\n","      <td>-0.049118</td>\n","      <td>0.032214</td>\n","      <td>0.060622</td>\n","      <td>-0.083585</td>\n","      <td>0.057289</td>\n","      <td>-0.029227</td>\n","      <td>0.123767</td>\n","      <td>-0.056181</td>\n","      <td>-0.009704</td>\n","      <td>-0.066307</td>\n","      <td>-0.054858</td>\n","      <td>0.022692</td>\n","      <td>-0.001024</td>\n","      <td>0.024198</td>\n","      <td>0.029573</td>\n","      <td>0.015123</td>\n","      <td>-0.015411</td>\n","      <td>-0.072242</td>\n","      <td>0.002403</td>\n","      <td>0.068810</td>\n","      <td>0.078108</td>\n","      <td>-0.114918</td>\n","      <td>0.172035</td>\n","      <td>-0.015467</td>\n","      <td>-0.065598</td>\n","      <td>-0.042662</td>\n","      <td>-0.122807</td>\n","      <td>0.055011</td>\n","      <td>0.125030</td>\n","      <td>0.042997</td>\n","      <td>-0.069575</td>\n","      <td>-0.019583</td>\n","      <td>0.083227</td>\n","      <td>-0.031093</td>\n","      <td>0.113242</td>\n","      <td>0.108977</td>\n","      <td>-0.075869</td>\n","      <td>-0.010822</td>\n","      <td>-0.023077</td>\n","      <td>-0.042996</td>\n","      <td>-0.086194</td>\n","      <td>0.041288</td>\n","      <td>0.029689</td>\n","      <td>-0.021416</td>\n","      <td>0.106496</td>\n","      <td>-0.032068</td>\n","      <td>0.106279</td>\n","      <td>0.041962</td>\n","      <td>-0.005453</td>\n","      <td>0.009421</td>\n","      <td>-0.032141</td>\n","      <td>-0.007000</td>\n","      <td>0.030459</td>\n","      <td>0.060638</td>\n","      <td>0.031381</td>\n","      <td>-0.077893</td>\n","      <td>0.088613</td>\n","      <td>-0.052895</td>\n","      <td>0.063891</td>\n","      <td>-0.020740</td>\n","      <td>-0.089413</td>\n","      <td>0.119845</td>\n","      <td>-0.037136</td>\n","      <td>0.078471</td>\n","      <td>0.142027</td>\n","      <td>-0.156986</td>\n","      <td>-0.018520</td>\n","      <td>-0.038021</td>\n","      <td>-0.056625</td>\n","      <td>0.048725</td>\n","      <td>0.132279</td>\n","      <td>-0.066204</td>\n","      <td>-0.051823</td>\n","      <td>-0.153481</td>\n","      <td>0.068772</td>\n","      <td>-0.062442</td>\n","      <td>-0.136036</td>\n","      <td>-0.110760</td>\n","      <td>-0.030329</td>\n","      <td>0.090953</td>\n","      <td>0.007290</td>\n","      <td>0.008732</td>\n","      <td>-0.039868</td>\n","      <td>-0.018497</td>\n","      <td>-0.060347</td>\n","      <td>-0.015938</td>\n","      <td>-0.021272</td>\n","      <td>0.013471</td>\n","      <td>0.074337</td>\n","      <td>-0.085318</td>\n","      <td>-0.013495</td>\n","      <td>-0.047985</td>\n","      <td>-0.101322</td>\n","      <td>0.030417</td>\n","      <td>-0.040203</td>\n","      <td>-0.061894</td>\n","      <td>-0.165288</td>\n","      <td>-0.008420</td>\n","      <td>0.031200</td>\n","      <td>0.017992</td>\n","      <td>-0.042058</td>\n","      <td>0.015014</td>\n","      <td>-0.115281</td>\n","      <td>-0.125116</td>\n","      <td>0.055632</td>\n","      <td>0.138109</td>\n","      <td>-0.055679</td>\n","      <td>-0.044133</td>\n","      <td>-0.034917</td>\n","      <td>-0.124181</td>\n","      <td>-0.050374</td>\n","      <td>-0.073679</td>\n","      <td>0.012048</td>\n","      <td>-0.021183</td>\n","      <td>0.092777</td>\n","      <td>0.077639</td>\n","      <td>0.116902</td>\n","      <td>0.063858</td>\n","      <td>0.050299</td>\n","      <td>0.077875</td>\n","      <td>-0.062970</td>\n","      <td>-0.163558</td>\n","      <td>0.025897</td>\n","      <td>0.018731</td>\n","      <td>0.010329</td>\n","      <td>-0.091666</td>\n","      <td>-0.023837</td>\n","      <td>-0.070894</td>\n","      <td>0.077318</td>\n","      <td>0.103459</td>\n","      <td>0.080595</td>\n","      <td>-0.063683</td>\n","      <td>0.039074</td>\n","      <td>0.048327</td>\n","      <td>-0.021850</td>\n","      <td>0.042156</td>\n","      <td>-0.189498</td>\n","      <td>0.112903</td>\n","      <td>-0.110578</td>\n","      <td>0.034369</td>\n","      <td>-0.010936</td>\n","      <td>-0.044027</td>\n","      <td>-0.007471</td>\n","      <td>0.034657</td>\n","      <td>-0.141182</td>\n","      <td>-0.024812</td>\n","      <td>-0.020043</td>\n","      <td>-0.022619</td>\n","      <td>-0.050388</td>\n","      <td>-0.054082</td>\n","      <td>-0.022264</td>\n","      <td>-0.016775</td>\n","      <td>0.024092</td>\n","      <td>-0.060166</td>\n","      <td>0.067930</td>\n","      <td>-0.108790</td>\n","      <td>0.034734</td>\n","      <td>0.026063</td>\n","      <td>0.048788</td>\n","      <td>0.032776</td>\n","      <td>-0.008451</td>\n","      <td>-0.015858</td>\n","      <td>-0.027340</td>\n","      <td>-0.009137</td>\n","      <td>0.017998</td>\n","      <td>-0.030767</td>\n","      <td>-0.015812</td>\n","      <td>-0.158525</td>\n","      <td>-0.069102</td>\n","      <td>-0.109487</td>\n","      <td>0.014108</td>\n","      <td>-0.052514</td>\n","      <td>0.084862</td>\n","      <td>0.086204</td>\n","      <td>0.037076</td>\n","      <td>0.057154</td>\n","      <td>0.010452</td>\n","      <td>0.079497</td>\n","      <td>0.047181</td>\n","      <td>-0.050245</td>\n","      <td>-0.107450</td>\n","      <td>0.053801</td>\n","      <td>0.092587</td>\n","      <td>0.066046</td>\n","      <td>-0.055236</td>\n","      <td>0.025235</td>\n","      <td>0.087963</td>\n","      <td>0.023907</td>\n","      <td>-0.018849</td>\n","      <td>0.011252</td>\n","      <td>0.046331</td>\n","      <td>0.034534</td>\n","      <td>-0.046726</td>\n","      <td>-0.048955</td>\n","      <td>-0.045833</td>\n","      <td>0.039325</td>\n","      <td>-0.035521</td>\n","      <td>0.003560</td>\n","      <td>0.038514</td>\n","      <td>-0.067373</td>\n","    </tr>\n","    <tr>\n","      <th>1150</th>\n","      <td>0.006235</td>\n","      <td>-0.017672</td>\n","      <td>0.012076</td>\n","      <td>-0.023884</td>\n","      <td>0.027323</td>\n","      <td>-0.070957</td>\n","      <td>0.078499</td>\n","      <td>-0.043815</td>\n","      <td>0.038350</td>\n","      <td>-0.036410</td>\n","      <td>0.009604</td>\n","      <td>0.006773</td>\n","      <td>-0.055715</td>\n","      <td>-0.027569</td>\n","      <td>0.029352</td>\n","      <td>-0.001701</td>\n","      <td>-0.039044</td>\n","      <td>-0.065815</td>\n","      <td>-0.038615</td>\n","      <td>0.009912</td>\n","      <td>0.027238</td>\n","      <td>-0.093028</td>\n","      <td>0.122101</td>\n","      <td>-0.012678</td>\n","      <td>-0.083448</td>\n","      <td>-0.027886</td>\n","      <td>-0.025582</td>\n","      <td>0.066163</td>\n","      <td>0.053645</td>\n","      <td>0.022592</td>\n","      <td>-0.096988</td>\n","      <td>0.004132</td>\n","      <td>0.011476</td>\n","      <td>-0.002053</td>\n","      <td>0.075432</td>\n","      <td>0.058798</td>\n","      <td>-0.042951</td>\n","      <td>0.006561</td>\n","      <td>0.020467</td>\n","      <td>-0.034189</td>\n","      <td>-0.091852</td>\n","      <td>0.011402</td>\n","      <td>0.016312</td>\n","      <td>-0.016245</td>\n","      <td>0.071578</td>\n","      <td>-0.117881</td>\n","      <td>0.085105</td>\n","      <td>-0.040460</td>\n","      <td>0.020860</td>\n","      <td>-0.004586</td>\n","      <td>0.034816</td>\n","      <td>-0.006695</td>\n","      <td>-0.025026</td>\n","      <td>0.041851</td>\n","      <td>0.082150</td>\n","      <td>-0.024918</td>\n","      <td>0.047881</td>\n","      <td>0.002375</td>\n","      <td>0.072377</td>\n","      <td>-0.009986</td>\n","      <td>-0.042939</td>\n","      <td>0.023726</td>\n","      <td>-0.023774</td>\n","      <td>0.049321</td>\n","      <td>0.066995</td>\n","      <td>-0.099016</td>\n","      <td>0.003178</td>\n","      <td>-0.002789</td>\n","      <td>-0.029774</td>\n","      <td>0.029743</td>\n","      <td>0.099259</td>\n","      <td>-0.018871</td>\n","      <td>-0.032803</td>\n","      <td>-0.100940</td>\n","      <td>-0.051197</td>\n","      <td>-0.033699</td>\n","      <td>-0.021397</td>\n","      <td>-0.073017</td>\n","      <td>-0.017945</td>\n","      <td>0.116594</td>\n","      <td>-0.044801</td>\n","      <td>-0.004404</td>\n","      <td>-0.004101</td>\n","      <td>0.007551</td>\n","      <td>0.032906</td>\n","      <td>-0.024762</td>\n","      <td>0.014490</td>\n","      <td>0.041603</td>\n","      <td>0.000617</td>\n","      <td>-0.014350</td>\n","      <td>-0.024465</td>\n","      <td>-0.032176</td>\n","      <td>-0.049087</td>\n","      <td>-0.045426</td>\n","      <td>0.023937</td>\n","      <td>0.027414</td>\n","      <td>-0.046047</td>\n","      <td>-0.029671</td>\n","      <td>-0.033038</td>\n","      <td>0.039193</td>\n","      <td>-0.017644</td>\n","      <td>0.019043</td>\n","      <td>-0.136795</td>\n","      <td>-0.108379</td>\n","      <td>0.051172</td>\n","      <td>0.096502</td>\n","      <td>-0.083516</td>\n","      <td>-0.029439</td>\n","      <td>-0.053367</td>\n","      <td>-0.082539</td>\n","      <td>-0.026817</td>\n","      <td>-0.051149</td>\n","      <td>-0.027030</td>\n","      <td>-0.068676</td>\n","      <td>0.107513</td>\n","      <td>0.059595</td>\n","      <td>0.074144</td>\n","      <td>0.011247</td>\n","      <td>-0.023399</td>\n","      <td>0.008572</td>\n","      <td>0.017543</td>\n","      <td>-0.074460</td>\n","      <td>0.110362</td>\n","      <td>0.064994</td>\n","      <td>0.001281</td>\n","      <td>-0.004883</td>\n","      <td>-0.038830</td>\n","      <td>-0.030212</td>\n","      <td>0.073502</td>\n","      <td>0.138487</td>\n","      <td>0.050746</td>\n","      <td>-0.105690</td>\n","      <td>-0.015533</td>\n","      <td>-0.014552</td>\n","      <td>-0.005388</td>\n","      <td>0.018643</td>\n","      <td>-0.132125</td>\n","      <td>0.104937</td>\n","      <td>-0.074165</td>\n","      <td>0.051478</td>\n","      <td>-0.048656</td>\n","      <td>-0.082441</td>\n","      <td>0.043412</td>\n","      <td>-0.017927</td>\n","      <td>-0.065976</td>\n","      <td>-0.006495</td>\n","      <td>0.029779</td>\n","      <td>-0.093904</td>\n","      <td>-0.051997</td>\n","      <td>-0.046039</td>\n","      <td>0.017327</td>\n","      <td>0.015099</td>\n","      <td>-0.005411</td>\n","      <td>-0.079389</td>\n","      <td>0.034067</td>\n","      <td>-0.033947</td>\n","      <td>0.092041</td>\n","      <td>-0.040583</td>\n","      <td>0.033964</td>\n","      <td>0.051664</td>\n","      <td>-0.040225</td>\n","      <td>0.027766</td>\n","      <td>-0.033302</td>\n","      <td>0.055144</td>\n","      <td>0.029050</td>\n","      <td>-0.014720</td>\n","      <td>-0.028231</td>\n","      <td>-0.077258</td>\n","      <td>-0.088284</td>\n","      <td>-0.045815</td>\n","      <td>-0.050350</td>\n","      <td>0.050392</td>\n","      <td>0.113562</td>\n","      <td>0.043499</td>\n","      <td>0.074122</td>\n","      <td>0.071969</td>\n","      <td>0.025724</td>\n","      <td>0.079470</td>\n","      <td>-0.053431</td>\n","      <td>-0.049001</td>\n","      <td>-0.076902</td>\n","      <td>-0.071199</td>\n","      <td>-0.050780</td>\n","      <td>0.095269</td>\n","      <td>-0.040761</td>\n","      <td>0.024497</td>\n","      <td>0.060562</td>\n","      <td>0.005410</td>\n","      <td>0.006360</td>\n","      <td>0.000422</td>\n","      <td>-0.027224</td>\n","      <td>0.001577</td>\n","      <td>-0.031546</td>\n","      <td>-0.036151</td>\n","      <td>0.010115</td>\n","      <td>0.029689</td>\n","      <td>-0.034936</td>\n","      <td>0.095174</td>\n","      <td>-0.034567</td>\n","      <td>-0.092490</td>\n","    </tr>\n","    <tr>\n","      <th>1151</th>\n","      <td>-0.022282</td>\n","      <td>0.029574</td>\n","      <td>0.066995</td>\n","      <td>-0.086589</td>\n","      <td>0.053718</td>\n","      <td>-0.040400</td>\n","      <td>0.094507</td>\n","      <td>-0.066640</td>\n","      <td>0.000890</td>\n","      <td>-0.034441</td>\n","      <td>-0.020217</td>\n","      <td>0.027315</td>\n","      <td>0.009674</td>\n","      <td>0.022458</td>\n","      <td>0.034359</td>\n","      <td>-0.019684</td>\n","      <td>0.015538</td>\n","      <td>-0.054322</td>\n","      <td>0.033923</td>\n","      <td>0.057314</td>\n","      <td>0.041472</td>\n","      <td>-0.113220</td>\n","      <td>0.155711</td>\n","      <td>-0.044806</td>\n","      <td>-0.091000</td>\n","      <td>-0.064378</td>\n","      <td>-0.098197</td>\n","      <td>0.035902</td>\n","      <td>0.094623</td>\n","      <td>0.014563</td>\n","      <td>-0.061029</td>\n","      <td>-0.012265</td>\n","      <td>0.060103</td>\n","      <td>-0.017041</td>\n","      <td>0.064684</td>\n","      <td>0.064092</td>\n","      <td>-0.057996</td>\n","      <td>-0.053506</td>\n","      <td>0.015399</td>\n","      <td>-0.005515</td>\n","      <td>-0.108750</td>\n","      <td>0.012258</td>\n","      <td>0.002656</td>\n","      <td>-0.040809</td>\n","      <td>0.112995</td>\n","      <td>-0.027821</td>\n","      <td>0.058676</td>\n","      <td>-0.013121</td>\n","      <td>-0.016493</td>\n","      <td>0.052438</td>\n","      <td>-0.021144</td>\n","      <td>0.027024</td>\n","      <td>-0.014248</td>\n","      <td>0.031780</td>\n","      <td>0.004234</td>\n","      <td>-0.085957</td>\n","      <td>0.032796</td>\n","      <td>-0.035570</td>\n","      <td>0.068755</td>\n","      <td>-0.056968</td>\n","      <td>-0.096011</td>\n","      <td>0.117818</td>\n","      <td>-0.046888</td>\n","      <td>0.058397</td>\n","      <td>0.106161</td>\n","      <td>-0.115394</td>\n","      <td>0.016777</td>\n","      <td>-0.080026</td>\n","      <td>-0.064426</td>\n","      <td>0.040655</td>\n","      <td>0.075999</td>\n","      <td>-0.047032</td>\n","      <td>-0.039097</td>\n","      <td>-0.087223</td>\n","      <td>0.043976</td>\n","      <td>-0.058802</td>\n","      <td>-0.092229</td>\n","      <td>-0.095970</td>\n","      <td>-0.004457</td>\n","      <td>0.077616</td>\n","      <td>0.019307</td>\n","      <td>0.032196</td>\n","      <td>-0.033232</td>\n","      <td>-0.010977</td>\n","      <td>-0.062224</td>\n","      <td>-0.011752</td>\n","      <td>-0.035967</td>\n","      <td>-0.012500</td>\n","      <td>-0.015070</td>\n","      <td>-0.083328</td>\n","      <td>-0.019759</td>\n","      <td>-0.085553</td>\n","      <td>-0.081460</td>\n","      <td>-0.001958</td>\n","      <td>-0.022902</td>\n","      <td>-0.075619</td>\n","      <td>-0.151668</td>\n","      <td>0.007394</td>\n","      <td>0.033471</td>\n","      <td>0.042532</td>\n","      <td>-0.057369</td>\n","      <td>0.044034</td>\n","      <td>-0.122481</td>\n","      <td>-0.106660</td>\n","      <td>0.036979</td>\n","      <td>0.105126</td>\n","      <td>-0.037096</td>\n","      <td>-0.038392</td>\n","      <td>-0.021773</td>\n","      <td>-0.053748</td>\n","      <td>-0.017457</td>\n","      <td>-0.060901</td>\n","      <td>-0.000261</td>\n","      <td>-0.013547</td>\n","      <td>0.077735</td>\n","      <td>0.084531</td>\n","      <td>0.090990</td>\n","      <td>0.085782</td>\n","      <td>0.018208</td>\n","      <td>0.091903</td>\n","      <td>-0.005485</td>\n","      <td>-0.137283</td>\n","      <td>0.049193</td>\n","      <td>0.015931</td>\n","      <td>0.055712</td>\n","      <td>-0.046126</td>\n","      <td>0.010465</td>\n","      <td>-0.069988</td>\n","      <td>0.049169</td>\n","      <td>0.089836</td>\n","      <td>0.103387</td>\n","      <td>-0.040312</td>\n","      <td>0.045891</td>\n","      <td>0.023947</td>\n","      <td>-0.018927</td>\n","      <td>0.058168</td>\n","      <td>-0.169372</td>\n","      <td>0.114719</td>\n","      <td>-0.087366</td>\n","      <td>0.001254</td>\n","      <td>-0.032718</td>\n","      <td>-0.046297</td>\n","      <td>-0.002542</td>\n","      <td>0.021964</td>\n","      <td>-0.116600</td>\n","      <td>-0.021783</td>\n","      <td>0.000838</td>\n","      <td>-0.028560</td>\n","      <td>-0.071247</td>\n","      <td>-0.037905</td>\n","      <td>0.012919</td>\n","      <td>-0.048715</td>\n","      <td>0.007704</td>\n","      <td>-0.023220</td>\n","      <td>0.054753</td>\n","      <td>-0.100323</td>\n","      <td>0.009580</td>\n","      <td>0.019336</td>\n","      <td>0.048080</td>\n","      <td>-0.004379</td>\n","      <td>0.002510</td>\n","      <td>-0.003263</td>\n","      <td>-0.034196</td>\n","      <td>0.004922</td>\n","      <td>0.026372</td>\n","      <td>0.020687</td>\n","      <td>0.007659</td>\n","      <td>-0.096587</td>\n","      <td>-0.043830</td>\n","      <td>-0.073482</td>\n","      <td>0.032236</td>\n","      <td>-0.034384</td>\n","      <td>0.077800</td>\n","      <td>0.066485</td>\n","      <td>0.056199</td>\n","      <td>0.070215</td>\n","      <td>0.024910</td>\n","      <td>0.083626</td>\n","      <td>0.055820</td>\n","      <td>-0.053145</td>\n","      <td>-0.100958</td>\n","      <td>0.056312</td>\n","      <td>0.067876</td>\n","      <td>0.062037</td>\n","      <td>-0.050352</td>\n","      <td>-0.016295</td>\n","      <td>0.076040</td>\n","      <td>0.006564</td>\n","      <td>-0.016717</td>\n","      <td>0.026991</td>\n","      <td>0.039636</td>\n","      <td>0.023284</td>\n","      <td>-0.010831</td>\n","      <td>-0.063653</td>\n","      <td>-0.045528</td>\n","      <td>0.005371</td>\n","      <td>-0.020269</td>\n","      <td>-0.007121</td>\n","      <td>0.023383</td>\n","      <td>-0.033246</td>\n","    </tr>\n","    <tr>\n","      <th>1152</th>\n","      <td>-0.035991</td>\n","      <td>0.023648</td>\n","      <td>0.083072</td>\n","      <td>-0.042938</td>\n","      <td>0.060768</td>\n","      <td>-0.009790</td>\n","      <td>0.138215</td>\n","      <td>-0.059506</td>\n","      <td>-0.009752</td>\n","      <td>-0.078840</td>\n","      <td>-0.067285</td>\n","      <td>-0.001763</td>\n","      <td>-0.063733</td>\n","      <td>0.018359</td>\n","      <td>0.009360</td>\n","      <td>-0.037832</td>\n","      <td>-0.019788</td>\n","      <td>-0.067800</td>\n","      <td>-0.029551</td>\n","      <td>0.011248</td>\n","      <td>0.040196</td>\n","      <td>-0.065747</td>\n","      <td>0.165956</td>\n","      <td>0.051016</td>\n","      <td>-0.039832</td>\n","      <td>-0.017549</td>\n","      <td>-0.105239</td>\n","      <td>0.065454</td>\n","      <td>0.047790</td>\n","      <td>0.050508</td>\n","      <td>-0.099091</td>\n","      <td>-0.004186</td>\n","      <td>0.013883</td>\n","      <td>-0.055413</td>\n","      <td>0.080570</td>\n","      <td>0.087494</td>\n","      <td>-0.002968</td>\n","      <td>-0.019430</td>\n","      <td>0.057478</td>\n","      <td>0.020379</td>\n","      <td>-0.115455</td>\n","      <td>-0.029989</td>\n","      <td>0.047968</td>\n","      <td>0.006784</td>\n","      <td>0.058064</td>\n","      <td>-0.073712</td>\n","      <td>0.074347</td>\n","      <td>0.011065</td>\n","      <td>0.060760</td>\n","      <td>-0.003289</td>\n","      <td>0.000510</td>\n","      <td>-0.054530</td>\n","      <td>0.019223</td>\n","      <td>0.046785</td>\n","      <td>0.029177</td>\n","      <td>-0.020246</td>\n","      <td>0.048640</td>\n","      <td>-0.022979</td>\n","      <td>0.033945</td>\n","      <td>0.000513</td>\n","      <td>-0.128178</td>\n","      <td>0.067518</td>\n","      <td>-0.082369</td>\n","      <td>0.037661</td>\n","      <td>0.057758</td>\n","      <td>-0.059157</td>\n","      <td>0.011298</td>\n","      <td>-0.032790</td>\n","      <td>-0.089826</td>\n","      <td>0.045623</td>\n","      <td>0.066122</td>\n","      <td>0.012389</td>\n","      <td>-0.023538</td>\n","      <td>-0.163593</td>\n","      <td>-0.030367</td>\n","      <td>-0.054545</td>\n","      <td>-0.067341</td>\n","      <td>-0.058988</td>\n","      <td>-0.028051</td>\n","      <td>0.084838</td>\n","      <td>0.033717</td>\n","      <td>0.026714</td>\n","      <td>0.001880</td>\n","      <td>-0.062166</td>\n","      <td>-0.040939</td>\n","      <td>0.011055</td>\n","      <td>0.012890</td>\n","      <td>-0.007463</td>\n","      <td>-0.002732</td>\n","      <td>-0.017736</td>\n","      <td>-0.026416</td>\n","      <td>-0.039449</td>\n","      <td>-0.040801</td>\n","      <td>-0.026294</td>\n","      <td>0.020017</td>\n","      <td>-0.007821</td>\n","      <td>-0.122790</td>\n","      <td>0.016752</td>\n","      <td>0.058723</td>\n","      <td>0.008190</td>\n","      <td>-0.022251</td>\n","      <td>0.008694</td>\n","      <td>-0.102262</td>\n","      <td>-0.102144</td>\n","      <td>0.081836</td>\n","      <td>0.148668</td>\n","      <td>-0.047490</td>\n","      <td>-0.020105</td>\n","      <td>-0.010636</td>\n","      <td>-0.082145</td>\n","      <td>-0.031902</td>\n","      <td>-0.041248</td>\n","      <td>0.011735</td>\n","      <td>-0.007152</td>\n","      <td>0.061280</td>\n","      <td>0.021813</td>\n","      <td>0.080436</td>\n","      <td>-0.006033</td>\n","      <td>0.018502</td>\n","      <td>0.052005</td>\n","      <td>0.015561</td>\n","      <td>-0.086223</td>\n","      <td>0.036729</td>\n","      <td>-0.013198</td>\n","      <td>0.077968</td>\n","      <td>-0.028588</td>\n","      <td>-0.020151</td>\n","      <td>-0.048056</td>\n","      <td>0.051410</td>\n","      <td>0.085085</td>\n","      <td>0.114491</td>\n","      <td>-0.072672</td>\n","      <td>-0.016825</td>\n","      <td>0.012191</td>\n","      <td>0.014775</td>\n","      <td>0.040866</td>\n","      <td>-0.154591</td>\n","      <td>0.084944</td>\n","      <td>-0.116173</td>\n","      <td>0.017703</td>\n","      <td>-0.002148</td>\n","      <td>-0.046886</td>\n","      <td>-0.025600</td>\n","      <td>0.011676</td>\n","      <td>-0.128949</td>\n","      <td>-0.050545</td>\n","      <td>-0.004086</td>\n","      <td>-0.075471</td>\n","      <td>-0.093504</td>\n","      <td>-0.007101</td>\n","      <td>-0.033082</td>\n","      <td>0.030299</td>\n","      <td>-0.019391</td>\n","      <td>-0.028513</td>\n","      <td>0.066767</td>\n","      <td>-0.114671</td>\n","      <td>0.005923</td>\n","      <td>0.036984</td>\n","      <td>0.048678</td>\n","      <td>0.071892</td>\n","      <td>0.026373</td>\n","      <td>0.005830</td>\n","      <td>0.064274</td>\n","      <td>0.019050</td>\n","      <td>0.086526</td>\n","      <td>-0.038704</td>\n","      <td>-0.005482</td>\n","      <td>-0.089162</td>\n","      <td>-0.079292</td>\n","      <td>-0.067506</td>\n","      <td>-0.018155</td>\n","      <td>0.009663</td>\n","      <td>0.047566</td>\n","      <td>0.095009</td>\n","      <td>0.003503</td>\n","      <td>0.019408</td>\n","      <td>-0.037114</td>\n","      <td>0.051756</td>\n","      <td>-0.002474</td>\n","      <td>0.001000</td>\n","      <td>-0.095223</td>\n","      <td>-0.005674</td>\n","      <td>0.042491</td>\n","      <td>0.055591</td>\n","      <td>-0.044142</td>\n","      <td>0.022217</td>\n","      <td>0.009330</td>\n","      <td>0.032165</td>\n","      <td>-0.043683</td>\n","      <td>0.044358</td>\n","      <td>0.037823</td>\n","      <td>0.041292</td>\n","      <td>-0.049212</td>\n","      <td>-0.027869</td>\n","      <td>-0.021177</td>\n","      <td>0.033536</td>\n","      <td>-0.047135</td>\n","      <td>0.022022</td>\n","      <td>0.002023</td>\n","      <td>-0.053826</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1153 rows Ã— 200 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6    \\\n","0     0.055312 -0.039930  0.191448 -0.149319  0.110794 -0.138439  0.051423   \n","1    -0.061018  0.016988  0.049502 -0.042556  0.012166  0.042974  0.143421   \n","2    -0.003201  0.049834  0.065356 -0.082197  0.084950 -0.005540  0.161819   \n","3    -0.029223  0.033038  0.016233 -0.010071 -0.024444 -0.029503  0.071805   \n","4     0.043262  0.043893  0.056917 -0.098647  0.092812 -0.017589  0.078938   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.025763  0.071045  0.037128 -0.129943  0.093425 -0.004798  0.139285   \n","1149 -0.049118  0.032214  0.060622 -0.083585  0.057289 -0.029227  0.123767   \n","1150  0.006235 -0.017672  0.012076 -0.023884  0.027323 -0.070957  0.078499   \n","1151 -0.022282  0.029574  0.066995 -0.086589  0.053718 -0.040400  0.094507   \n","1152 -0.035991  0.023648  0.083072 -0.042938  0.060768 -0.009790  0.138215   \n","\n","           7         8         9         10        11        12        13   \\\n","0    -0.003943 -0.007598 -0.065237  0.019513 -0.102994 -0.031106  0.056399   \n","1    -0.050513 -0.014999 -0.021124 -0.103508 -0.014746 -0.062674  0.039278   \n","2    -0.071908 -0.037254 -0.009837 -0.063657  0.012028 -0.054765  0.032465   \n","3    -0.021355 -0.013895  0.017048 -0.013460 -0.023246 -0.118383  0.059493   \n","4    -0.052581  0.008699 -0.016099 -0.023499  0.021725  0.067992 -0.026848   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.073269  0.022682  0.003862 -0.024284  0.065725  0.033895  0.024953   \n","1149 -0.056181 -0.009704 -0.066307 -0.054858  0.022692 -0.001024  0.024198   \n","1150 -0.043815  0.038350 -0.036410  0.009604  0.006773 -0.055715 -0.027569   \n","1151 -0.066640  0.000890 -0.034441 -0.020217  0.027315  0.009674  0.022458   \n","1152 -0.059506 -0.009752 -0.078840 -0.067285 -0.001763 -0.063733  0.018359   \n","\n","           14        15        16        17        18        19        20   \\\n","0     0.085243 -0.045452 -0.049448 -0.090799  0.057514  0.027814  0.194963   \n","1    -0.023490 -0.005170 -0.032101 -0.067672 -0.047502  0.086716  0.029468   \n","2     0.004423 -0.055838 -0.060516 -0.075242 -0.031401  0.052626  0.052314   \n","3     0.034331  0.009509 -0.087391 -0.008946 -0.027795  0.109961 -0.019577   \n","4     0.022914 -0.072122 -0.065780 -0.069934  0.019467  0.046615  0.036055   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.002252  0.014924 -0.017935 -0.072112 -0.028729  0.066526  0.037607   \n","1149  0.029573  0.015123 -0.015411 -0.072242  0.002403  0.068810  0.078108   \n","1150  0.029352 -0.001701 -0.039044 -0.065815 -0.038615  0.009912  0.027238   \n","1151  0.034359 -0.019684  0.015538 -0.054322  0.033923  0.057314  0.041472   \n","1152  0.009360 -0.037832 -0.019788 -0.067800 -0.029551  0.011248  0.040196   \n","\n","           21        22        23        24        25        26        27   \\\n","0    -0.144012  0.232128  0.004772 -0.113581  0.028224 -0.103949  0.111155   \n","1    -0.049504  0.094705  0.053743 -0.040958 -0.005397 -0.128751  0.031853   \n","2    -0.049840  0.120136  0.079043 -0.048444 -0.025812 -0.128978  0.043248   \n","3    -0.000089 -0.002808  0.040342 -0.063464 -0.025714 -0.052038  0.011922   \n","4    -0.052760  0.086948  0.067870 -0.012917 -0.021081 -0.091686  0.005725   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.099614  0.171433 -0.030529  0.008454  0.002131 -0.093959  0.073591   \n","1149 -0.114918  0.172035 -0.015467 -0.065598 -0.042662 -0.122807  0.055011   \n","1150 -0.093028  0.122101 -0.012678 -0.083448 -0.027886 -0.025582  0.066163   \n","1151 -0.113220  0.155711 -0.044806 -0.091000 -0.064378 -0.098197  0.035902   \n","1152 -0.065747  0.165956  0.051016 -0.039832 -0.017549 -0.105239  0.065454   \n","\n","           28        29        30        31        32        33        34   \\\n","0     0.073251  0.021104 -0.152625  0.039723  0.033198 -0.007666  0.077760   \n","1     0.037736  0.099800 -0.058445 -0.102526  0.049032 -0.041298  0.129646   \n","2     0.032008  0.110779 -0.100398 -0.016027  0.030733 -0.025223  0.096444   \n","3    -0.017488  0.124574 -0.083677 -0.114848  0.070005  0.001270  0.132067   \n","4    -0.007029  0.068535 -0.032816  0.046015  0.058832  0.015537 -0.004533   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.117216  0.081128 -0.061986 -0.011908  0.097863  0.072944  0.090765   \n","1149  0.125030  0.042997 -0.069575 -0.019583  0.083227 -0.031093  0.113242   \n","1150  0.053645  0.022592 -0.096988  0.004132  0.011476 -0.002053  0.075432   \n","1151  0.094623  0.014563 -0.061029 -0.012265  0.060103 -0.017041  0.064684   \n","1152  0.047790  0.050508 -0.099091 -0.004186  0.013883 -0.055413  0.080570   \n","\n","           35        36        37        38        39        40        41   \\\n","0    -0.008866 -0.102105 -0.035564  0.014161  0.097684 -0.174950  0.006351   \n","1     0.103363 -0.021137 -0.025913  0.037137 -0.038500 -0.128343 -0.006980   \n","2     0.093381 -0.024996 -0.037410  0.049682  0.014305 -0.106185 -0.029681   \n","3     0.039418 -0.025030 -0.047605 -0.019201 -0.056250 -0.076681  0.052190   \n","4     0.016547 -0.073101 -0.088147  0.039370  0.048142 -0.019322 -0.097676   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.187357 -0.140266 -0.026925  0.030918 -0.071531 -0.056693  0.057678   \n","1149  0.108977 -0.075869 -0.010822 -0.023077 -0.042996 -0.086194  0.041288   \n","1150  0.058798 -0.042951  0.006561  0.020467 -0.034189 -0.091852  0.011402   \n","1151  0.064092 -0.057996 -0.053506  0.015399 -0.005515 -0.108750  0.012258   \n","1152  0.087494 -0.002968 -0.019430  0.057478  0.020379 -0.115455 -0.029989   \n","\n","           42        43        44        45        46        47        48   \\\n","0     0.067718 -0.144566  0.244559 -0.055779  0.055129 -0.174438  0.053647   \n","1     0.078419  0.011923  0.009650 -0.093427  0.108384  0.033260  0.045413   \n","2     0.040773 -0.003502  0.059676 -0.082905  0.118139  0.008984  0.080412   \n","3     0.003089 -0.014640  0.045362 -0.131532  0.102838  0.024087  0.091748   \n","4     0.041618 -0.011262  0.069167 -0.044980  0.106995 -0.024451 -0.006377   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.033244  0.058875  0.034093 -0.071130  0.182654  0.057474 -0.013123   \n","1149  0.029689 -0.021416  0.106496 -0.032068  0.106279  0.041962 -0.005453   \n","1150  0.016312 -0.016245  0.071578 -0.117881  0.085105 -0.040460  0.020860   \n","1151  0.002656 -0.040809  0.112995 -0.027821  0.058676 -0.013121 -0.016493   \n","1152  0.047968  0.006784  0.058064 -0.073712  0.074347  0.011065  0.060760   \n","\n","           49        50        51        52        53        54        55   \\\n","0     0.138934  0.128230 -0.037576 -0.095177  0.096715 -0.029224 -0.147459   \n","1    -0.094165 -0.043469 -0.053775  0.057963  0.072712  0.043468 -0.044517   \n","2    -0.006643 -0.014248 -0.062612  0.001453  0.065533  0.036103 -0.093621   \n","3    -0.113966 -0.054380 -0.008877 -0.023470  0.016027  0.072124 -0.068161   \n","4     0.035807  0.022758 -0.071449 -0.040083  0.037616 -0.036219 -0.097986   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.006828 -0.027016 -0.069627  0.038774  0.038216  0.051217 -0.062813   \n","1149  0.009421 -0.032141 -0.007000  0.030459  0.060638  0.031381 -0.077893   \n","1150 -0.004586  0.034816 -0.006695 -0.025026  0.041851  0.082150 -0.024918   \n","1151  0.052438 -0.021144  0.027024 -0.014248  0.031780  0.004234 -0.085957   \n","1152 -0.003289  0.000510 -0.054530  0.019223  0.046785  0.029177 -0.020246   \n","\n","           56        57        58        59        60        61        62   \\\n","0    -0.039733 -0.081511  0.097535 -0.127752 -0.223355  0.064697 -0.067259   \n","1     0.075127 -0.026359  0.006359  0.043224 -0.153530  0.080685 -0.070069   \n","2     0.044886 -0.023221 -0.040951  0.020021 -0.070897  0.058370 -0.095237   \n","3    -0.004273 -0.014254 -0.047378  0.041782 -0.113214  0.022322 -0.037948   \n","4    -0.035913 -0.033559 -0.045972 -0.006644 -0.003945  0.045789 -0.043402   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.126596  0.002623  0.023030  0.071060  0.021068  0.072009  0.056083   \n","1149  0.088613 -0.052895  0.063891 -0.020740 -0.089413  0.119845 -0.037136   \n","1150  0.047881  0.002375  0.072377 -0.009986 -0.042939  0.023726 -0.023774   \n","1151  0.032796 -0.035570  0.068755 -0.056968 -0.096011  0.117818 -0.046888   \n","1152  0.048640 -0.022979  0.033945  0.000513 -0.128178  0.067518 -0.082369   \n","\n","           63        64        65        66        67        68        69   \\\n","0     0.072545  0.061619 -0.150009  0.152331 -0.194776 -0.020112 -0.021461   \n","1     0.052864  0.091646 -0.095059 -0.055839 -0.043282 -0.063824  0.066563   \n","2    -0.006513  0.072156 -0.061076  0.004190 -0.082927 -0.100370  0.002773   \n","3     0.081923  0.126138 -0.047886 -0.095410 -0.044869 -0.126054  0.058415   \n","4    -0.048720  0.029097 -0.006938  0.084531 -0.043571 -0.023949 -0.084357   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.000626  0.064103 -0.120418 -0.032645 -0.031961 -0.071685  0.000026   \n","1149  0.078471  0.142027 -0.156986 -0.018520 -0.038021 -0.056625  0.048725   \n","1150  0.049321  0.066995 -0.099016  0.003178 -0.002789 -0.029774  0.029743   \n","1151  0.058397  0.106161 -0.115394  0.016777 -0.080026 -0.064426  0.040655   \n","1152  0.037661  0.057758 -0.059157  0.011298 -0.032790 -0.089826  0.045623   \n","\n","           70        71        72        73        74        75        76   \\\n","0     0.087508  0.003023 -0.113706 -0.124695 -0.028077  0.069431 -0.201233   \n","1     0.056411 -0.015844 -0.008193 -0.215822  0.040523 -0.049753 -0.111441   \n","2     0.075639  0.028037 -0.016752 -0.231114 -0.002973 -0.057661 -0.078610   \n","3     0.054089  0.064041 -0.000390 -0.169077  0.069603 -0.046155 -0.017805   \n","4     0.009316  0.004477  0.020502 -0.118127 -0.043620 -0.027257 -0.038873   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.123615 -0.044415 -0.015697 -0.246760  0.011643 -0.094024 -0.129853   \n","1149  0.132279 -0.066204 -0.051823 -0.153481  0.068772 -0.062442 -0.136036   \n","1150  0.099259 -0.018871 -0.032803 -0.100940 -0.051197 -0.033699 -0.021397   \n","1151  0.075999 -0.047032 -0.039097 -0.087223  0.043976 -0.058802 -0.092229   \n","1152  0.066122  0.012389 -0.023538 -0.163593 -0.030367 -0.054545 -0.067341   \n","\n","           77        78        79        80        81        82        83   \\\n","0    -0.066189  0.035115  0.140000 -0.047706  0.146456 -0.048373 -0.056729   \n","1    -0.091611 -0.029417  0.086315  0.032174 -0.005204  0.040144 -0.071306   \n","2    -0.091225 -0.000109  0.132210  0.059829  0.031464  0.046621 -0.082754   \n","3    -0.039797  0.001620  0.113422  0.002681 -0.005993  0.016260 -0.000945   \n","4    -0.044611  0.030928  0.083007  0.064508  0.046536  0.066158 -0.057566   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.125414 -0.093487  0.190303  0.075941 -0.030443 -0.016166 -0.068983   \n","1149 -0.110760 -0.030329  0.090953  0.007290  0.008732 -0.039868 -0.018497   \n","1150 -0.073017 -0.017945  0.116594 -0.044801 -0.004404 -0.004101  0.007551   \n","1151 -0.095970 -0.004457  0.077616  0.019307  0.032196 -0.033232 -0.010977   \n","1152 -0.058988 -0.028051  0.084838  0.033717  0.026714  0.001880 -0.062166   \n","\n","           84        85        86        87        88        89        90   \\\n","0     0.041842  0.074862 -0.058753 -0.059540 -0.070898 -0.061751 -0.051123   \n","1    -0.071806 -0.002281  0.022584  0.052517  0.117852 -0.008148 -0.030722   \n","2    -0.034180  0.047502  0.002053  0.009119  0.010232 -0.003966 -0.050242   \n","3    -0.043262 -0.041749  0.011798  0.030224  0.045672  0.039913 -0.021678   \n","4    -0.014875  0.081208  0.009624  0.023848 -0.077927 -0.024666 -0.066208   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.046988 -0.014944 -0.005712  0.049873  0.090093 -0.106615 -0.028315   \n","1149 -0.060347 -0.015938 -0.021272  0.013471  0.074337 -0.085318 -0.013495   \n","1150  0.032906 -0.024762  0.014490  0.041603  0.000617 -0.014350 -0.024465   \n","1151 -0.062224 -0.011752 -0.035967 -0.012500 -0.015070 -0.083328 -0.019759   \n","1152 -0.040939  0.011055  0.012890 -0.007463 -0.002732 -0.017736 -0.026416   \n","\n","           91        92        93        94        95        96        97   \\\n","0    -0.182045 -0.160678 -0.065777  0.074163 -0.073466 -0.267037 -0.158344   \n","1    -0.027184 -0.027622  0.034868  0.004980 -0.042590 -0.114096  0.058591   \n","2    -0.056497 -0.046894 -0.024984  0.019060 -0.027742 -0.112245 -0.023793   \n","3    -0.093443  0.020153 -0.041662  0.080349 -0.014209 -0.014459  0.005705   \n","4    -0.070249 -0.054624 -0.032657  0.033347 -0.009000 -0.085000 -0.052862   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.026453 -0.100344  0.050182 -0.057447 -0.077865 -0.128419 -0.034301   \n","1149 -0.047985 -0.101322  0.030417 -0.040203 -0.061894 -0.165288 -0.008420   \n","1150 -0.032176 -0.049087 -0.045426  0.023937  0.027414 -0.046047 -0.029671   \n","1151 -0.085553 -0.081460 -0.001958 -0.022902 -0.075619 -0.151668  0.007394   \n","1152 -0.039449 -0.040801 -0.026294  0.020017 -0.007821 -0.122790  0.016752   \n","\n","           98        99        100       101       102       103       104  \\\n","0     0.044062  0.051425 -0.077031  0.114817 -0.182484 -0.070219  0.006282   \n","1     0.055083 -0.052130 -0.016828 -0.018380 -0.044166 -0.114783  0.082226   \n","2     0.006148  0.025831 -0.026951 -0.032982 -0.043011 -0.087830  0.060428   \n","3    -0.016403  0.001628 -0.029381 -0.045193 -0.032422 -0.056614  0.023511   \n","4    -0.024535  0.025881 -0.052433  0.012753 -0.011142 -0.022926 -0.029888   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.052331  0.061794  0.003536 -0.071607 -0.048121 -0.089022 -0.006083   \n","1149  0.031200  0.017992 -0.042058  0.015014 -0.115281 -0.125116  0.055632   \n","1150 -0.033038  0.039193 -0.017644  0.019043 -0.136795 -0.108379  0.051172   \n","1151  0.033471  0.042532 -0.057369  0.044034 -0.122481 -0.106660  0.036979   \n","1152  0.058723  0.008190 -0.022251  0.008694 -0.102262 -0.102144  0.081836   \n","\n","           105       106       107       108       109       110       111  \\\n","0     0.098775 -0.144713 -0.040478 -0.011775 -0.024338 -0.004593 -0.090838   \n","1     0.158211 -0.062394  0.001910 -0.037969 -0.100493 -0.035638 -0.011532   \n","2     0.132168 -0.018053 -0.048712 -0.017970 -0.008626 -0.007273 -0.005247   \n","3     0.141625 -0.021386  0.021874 -0.026763  0.005500 -0.024027  0.084337   \n","4     0.021118  0.071407 -0.083502 -0.013588  0.082711  0.057966 -0.056847   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.088820 -0.022778 -0.051379 -0.058044 -0.063221 -0.001680 -0.028314   \n","1149  0.138109 -0.055679 -0.044133 -0.034917 -0.124181 -0.050374 -0.073679   \n","1150  0.096502 -0.083516 -0.029439 -0.053367 -0.082539 -0.026817 -0.051149   \n","1151  0.105126 -0.037096 -0.038392 -0.021773 -0.053748 -0.017457 -0.060901   \n","1152  0.148668 -0.047490 -0.020105 -0.010636 -0.082145 -0.031902 -0.041248   \n","\n","           112       113       114       115       116       117       118  \\\n","0    -0.007223  0.002444 -0.090104  0.087139  0.028637  0.114386 -0.059107   \n","1     0.055595  0.023990  0.119399 -0.001851  0.071994 -0.086471  0.092134   \n","2     0.023876 -0.010678  0.069116  0.035054  0.050647 -0.039136  0.019373   \n","3     0.053580 -0.016007  0.078336  0.039094  0.070058 -0.123502  0.026837   \n","4     0.018287 -0.001665 -0.024429  0.032644 -0.043036  0.024678 -0.045200   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.003729 -0.033152  0.089003  0.041804  0.021812  0.056624  0.030852   \n","1149  0.012048 -0.021183  0.092777  0.077639  0.116902  0.063858  0.050299   \n","1150 -0.027030 -0.068676  0.107513  0.059595  0.074144  0.011247 -0.023399   \n","1151 -0.000261 -0.013547  0.077735  0.084531  0.090990  0.085782  0.018208   \n","1152  0.011735 -0.007152  0.061280  0.021813  0.080436 -0.006033  0.018502   \n","\n","           119       120       121       122       123       124       125  \\\n","0     0.314040  0.036433 -0.207628  0.121528  0.168863 -0.030016 -0.089476   \n","1    -0.022091 -0.041771 -0.097996 -0.017188  0.001062  0.062520 -0.069970   \n","2     0.026349  0.011268 -0.048878  0.026545 -0.018460  0.089342 -0.034219   \n","3    -0.048717 -0.017742 -0.109955 -0.013687  0.064595  0.011309 -0.006182   \n","4     0.063647  0.049025  0.019913  0.066489 -0.025849  0.028416  0.010319   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.023005 -0.112521 -0.052762  0.082858  0.018961  0.065693 -0.035522   \n","1149  0.077875 -0.062970 -0.163558  0.025897  0.018731  0.010329 -0.091666   \n","1150  0.008572  0.017543 -0.074460  0.110362  0.064994  0.001281 -0.004883   \n","1151  0.091903 -0.005485 -0.137283  0.049193  0.015931  0.055712 -0.046126   \n","1152  0.052005  0.015561 -0.086223  0.036729 -0.013198  0.077968 -0.028588   \n","\n","           126       127       128       129       130       131       132  \\\n","0    -0.056296 -0.229768  0.038695  0.229738  0.060928  0.029991  0.077916   \n","1    -0.066527 -0.044718  0.074902 -0.002593  0.096486 -0.111685 -0.041615   \n","2    -0.034568 -0.053844  0.011709  0.069690  0.127932 -0.058237 -0.069246   \n","3    -0.093979  0.006954  0.026299 -0.047437  0.064258 -0.076026 -0.065328   \n","4     0.002876 -0.085854 -0.083701  0.077935  0.054166  0.018909 -0.019035   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.018624 -0.027916  0.056331  0.139032  0.067437 -0.075481  0.006476   \n","1149 -0.023837 -0.070894  0.077318  0.103459  0.080595 -0.063683  0.039074   \n","1150 -0.038830 -0.030212  0.073502  0.138487  0.050746 -0.105690 -0.015533   \n","1151  0.010465 -0.069988  0.049169  0.089836  0.103387 -0.040312  0.045891   \n","1152 -0.020151 -0.048056  0.051410  0.085085  0.114491 -0.072672 -0.016825   \n","\n","           133       134       135       136       137       138       139  \\\n","0     0.095506 -0.088583  0.146278 -0.135664  0.075616 -0.050393  0.051478   \n","1     0.026480 -0.017918  0.017210 -0.164579  0.100271 -0.208402  0.032018   \n","2     0.026614 -0.012108  0.066991 -0.121714  0.061513 -0.115687  0.053551   \n","3    -0.035491 -0.034816 -0.004179 -0.081492  0.088453 -0.194769  0.118167   \n","4     0.080205 -0.016994  0.112154 -0.012276 -0.002677  0.033562  0.007683   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.021048 -0.045508  0.095951 -0.122612  0.085910 -0.046145  0.042162   \n","1149  0.048327 -0.021850  0.042156 -0.189498  0.112903 -0.110578  0.034369   \n","1150 -0.014552 -0.005388  0.018643 -0.132125  0.104937 -0.074165  0.051478   \n","1151  0.023947 -0.018927  0.058168 -0.169372  0.114719 -0.087366  0.001254   \n","1152  0.012191  0.014775  0.040866 -0.154591  0.084944 -0.116173  0.017703   \n","\n","           140       141       142       143       144       145       146  \\\n","0    -0.004462 -0.191311 -0.031969 -0.002654 -0.121763 -0.110812  0.090307   \n","1     0.023155  0.037884 -0.026814  0.044153 -0.153497 -0.021794 -0.028157   \n","2     0.002006 -0.028892 -0.054182  0.040983 -0.158888 -0.035364  0.004313   \n","3     0.013936  0.040369 -0.035155  0.048860 -0.050524  0.022171 -0.033376   \n","4    -0.062404 -0.034798 -0.015451  0.019375 -0.116244 -0.031569  0.049624   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.042910 -0.049483  0.044537 -0.016423 -0.187987  0.004531  0.010061   \n","1149 -0.010936 -0.044027 -0.007471  0.034657 -0.141182 -0.024812 -0.020043   \n","1150 -0.048656 -0.082441  0.043412 -0.017927 -0.065976 -0.006495  0.029779   \n","1151 -0.032718 -0.046297 -0.002542  0.021964 -0.116600 -0.021783  0.000838   \n","1152 -0.002148 -0.046886 -0.025600  0.011676 -0.128949 -0.050545 -0.004086   \n","\n","           147       148       149       150       151       152       153  \\\n","0    -0.082125 -0.254773 -0.064707 -0.022161 -0.154172 -0.137295 -0.059365   \n","1    -0.038355 -0.059844 -0.019138 -0.086807  0.033628 -0.007282 -0.051670   \n","2    -0.097934 -0.111168 -0.054865 -0.039240  0.059383 -0.037972 -0.051662   \n","3    -0.109460 -0.088010 -0.008427 -0.107055 -0.011149 -0.099718 -0.065303   \n","4    -0.021415 -0.088397 -0.059330  0.037670  0.020383 -0.017984 -0.025215   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.069784  0.085104 -0.072495 -0.049618  0.100725  0.048605 -0.096432   \n","1149 -0.022619 -0.050388 -0.054082 -0.022264 -0.016775  0.024092 -0.060166   \n","1150 -0.093904 -0.051997 -0.046039  0.017327  0.015099 -0.005411 -0.079389   \n","1151 -0.028560 -0.071247 -0.037905  0.012919 -0.048715  0.007704 -0.023220   \n","1152 -0.075471 -0.093504 -0.007101 -0.033082  0.030299 -0.019391 -0.028513   \n","\n","           154       155       156       157       158       159       160  \\\n","0     0.067794 -0.069797 -0.033828 -0.001323  0.008541  0.094177 -0.028051   \n","1     0.044085 -0.115107  0.009230  0.029948  0.043391  0.081878  0.021124   \n","2     0.058816 -0.136445  0.052160  0.056786  0.047382  0.039975 -0.009758   \n","3    -0.013957 -0.089034  0.107907 -0.039467  0.037774  0.082743  0.027358   \n","4     0.012125 -0.093454  0.035094  0.058969 -0.014608 -0.058347  0.010617   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.065582 -0.120835  0.127561  0.037443 -0.000482  0.020684 -0.021025   \n","1149  0.067930 -0.108790  0.034734  0.026063  0.048788  0.032776 -0.008451   \n","1150  0.034067 -0.033947  0.092041 -0.040583  0.033964  0.051664 -0.040225   \n","1151  0.054753 -0.100323  0.009580  0.019336  0.048080 -0.004379  0.002510   \n","1152  0.066767 -0.114671  0.005923  0.036984  0.048678  0.071892  0.026373   \n","\n","           161       162       163       164       165       166       167  \\\n","0    -0.015630 -0.009411  0.041273  0.160046 -0.120427  0.027131 -0.032987   \n","1     0.009889  0.062498  0.015040  0.014968  0.013056 -0.045615 -0.188553   \n","2     0.000636  0.062056  0.016325  0.084557 -0.015950 -0.018937 -0.135114   \n","3     0.012457  0.048642  0.124568  0.000198  0.057655 -0.040044 -0.209324   \n","4    -0.016558  0.039073 -0.066809  0.052841 -0.028467  0.019787  0.000242   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.035419  0.003044 -0.025148 -0.011260 -0.070878 -0.085629 -0.121183   \n","1149 -0.015858 -0.027340 -0.009137  0.017998 -0.030767 -0.015812 -0.158525   \n","1150  0.027766 -0.033302  0.055144  0.029050 -0.014720 -0.028231 -0.077258   \n","1151 -0.003263 -0.034196  0.004922  0.026372  0.020687  0.007659 -0.096587   \n","1152  0.005830  0.064274  0.019050  0.086526 -0.038704 -0.005482 -0.089162   \n","\n","           168       169       170       171       172       173       174  \\\n","0    -0.096873 -0.113636 -0.062230 -0.015315  0.050063  0.215760  0.024331   \n","1    -0.074055 -0.081047  0.007862 -0.012802  0.060702  0.092859  0.011237   \n","2    -0.052426 -0.051657 -0.007431  0.016893  0.063027  0.097134  0.007296   \n","3    -0.028382 -0.000596 -0.014994  0.044176  0.119730  0.023371  0.008801   \n","4    -0.038667 -0.037864 -0.003309  0.004975 -0.003108  0.107157 -0.033303   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.051895 -0.078092 -0.004124 -0.027498  0.124604  0.056136  0.055589   \n","1149 -0.069102 -0.109487  0.014108 -0.052514  0.084862  0.086204  0.037076   \n","1150 -0.088284 -0.045815 -0.050350  0.050392  0.113562  0.043499  0.074122   \n","1151 -0.043830 -0.073482  0.032236 -0.034384  0.077800  0.066485  0.056199   \n","1152 -0.079292 -0.067506 -0.018155  0.009663  0.047566  0.095009  0.003503   \n","\n","           175       176       177       178       179       180       181  \\\n","0     0.083756  0.120729  0.155200  0.004358  0.013040 -0.203371  0.086246   \n","1     0.004092 -0.036368  0.031882  0.022227  0.021023 -0.102012 -0.057878   \n","2     0.040619 -0.063901  0.019059  0.003622 -0.008092 -0.108846  0.005799   \n","3    -0.044613 -0.027303  0.005224  0.005985  0.081936 -0.051553 -0.136268   \n","4     0.084484 -0.039878 -0.053463 -0.008048 -0.085443 -0.037588  0.043686   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.062818 -0.076826  0.014304 -0.072733 -0.111488 -0.018519  0.071011   \n","1149  0.057154  0.010452  0.079497  0.047181 -0.050245 -0.107450  0.053801   \n","1150  0.071969  0.025724  0.079470 -0.053431 -0.049001 -0.076902 -0.071199   \n","1151  0.070215  0.024910  0.083626  0.055820 -0.053145 -0.100958  0.056312   \n","1152  0.019408 -0.037114  0.051756 -0.002474  0.001000 -0.095223 -0.005674   \n","\n","           182       183       184       185       186       187       188  \\\n","0     0.128653  0.075776 -0.005553 -0.010128 -0.016698 -0.009678 -0.069997   \n","1     0.048686  0.085081 -0.030369  0.025377  0.032599 -0.000141 -0.025085   \n","2     0.059813  0.059305 -0.040137 -0.002679 -0.027258  0.066232 -0.055880   \n","3    -0.014287  0.077905 -0.045905 -0.051808 -0.004229 -0.054282  0.021064   \n","4     0.073742  0.012074 -0.027139 -0.033541 -0.077054  0.087032 -0.057262   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148  0.032206  0.107525 -0.071622  0.044711  0.043170  0.086394  0.022205   \n","1149  0.092587  0.066046 -0.055236  0.025235  0.087963  0.023907 -0.018849   \n","1150 -0.050780  0.095269 -0.040761  0.024497  0.060562  0.005410  0.006360   \n","1151  0.067876  0.062037 -0.050352 -0.016295  0.076040  0.006564 -0.016717   \n","1152  0.042491  0.055591 -0.044142  0.022217  0.009330  0.032165 -0.043683   \n","\n","           189       190       191       192       193       194       195  \\\n","0     0.136486  0.026843 -0.032314  0.014119 -0.067477 -0.133395 -0.078424   \n","1     0.017068  0.017619  0.026190 -0.089510 -0.028861  0.039401  0.096074   \n","2     0.029591  0.043968  0.024549 -0.044297 -0.019310 -0.005018  0.034421   \n","3    -0.064778 -0.023319 -0.011133 -0.087044 -0.022872  0.089331  0.092386   \n","4     0.041101  0.030200 -0.024849  0.010870 -0.008267  0.004219 -0.031360   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","1148 -0.013824  0.003970  0.058073 -0.036994 -0.010182 -0.019982  0.016088   \n","1149  0.011252  0.046331  0.034534 -0.046726 -0.048955 -0.045833  0.039325   \n","1150  0.000422 -0.027224  0.001577 -0.031546 -0.036151  0.010115  0.029689   \n","1151  0.026991  0.039636  0.023284 -0.010831 -0.063653 -0.045528  0.005371   \n","1152  0.044358  0.037823  0.041292 -0.049212 -0.027869 -0.021177  0.033536   \n","\n","           196       197       198       199  \n","0     0.032957  0.046098  0.003512 -0.021471  \n","1    -0.070603  0.047572 -0.006124 -0.083557  \n","2    -0.040792  0.029748  0.004514 -0.069791  \n","3    -0.026935  0.097230 -0.034497 -0.073435  \n","4     0.053717  0.022858  0.001484 -0.025883  \n","...        ...       ...       ...       ...  \n","1148 -0.021323  0.099319 -0.008026 -0.114344  \n","1149 -0.035521  0.003560  0.038514 -0.067373  \n","1150 -0.034936  0.095174 -0.034567 -0.092490  \n","1151 -0.020269 -0.007121  0.023383 -0.033246  \n","1152 -0.047135  0.022022  0.002023 -0.053826  \n","\n","[1153 rows x 200 columns]"]},"metadata":{"tags":[]},"execution_count":153}]},{"cell_type":"code","metadata":{"id":"1BPeEkijTqH9"},"source":["train_d2v = docvec_df.iloc[:1153,:]\n","test_d2v = docvec_df.iloc[1153:,:] \n","\n","# splitting data into training and validation set \n","xtrain_d2v, xvalid_d2v, ytrain, yvalid = train_test_split(train_d2v, dataset['class'], random_state=42, test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jIu5iIY1T5tq","outputId":"f2f0f8dd-177d-4049-be8f-ea9c90fe5258"},"source":["svc = svm.SVC(kernel='rbf', C=1).fit(xtrain_d2v, ytrain) \n","prediction = svc.predict(xvalid_d2v) \n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc1=accuracy_score(yvalid,prediction)\n","\n","print(\"SVM-RBF(doc2vec), Accuracy Score:\",acc1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.79      0.99      0.88       172\n","           1       0.93      0.22      0.36        59\n","\n","    accuracy                           0.80       231\n","   macro avg       0.86      0.61      0.62       231\n","weighted avg       0.82      0.80      0.75       231\n","\n","SVM-RBF(doc2vec), Accuracy Score: 0.7965367965367965\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dK95hg4JUrrO","outputId":"39fd4fe5-3e80-46a8-acd1-60f57270f186"},"source":["parameters = [{'C': [1,10,50,100], 'kernel': ['linear','rbf']}]\n","grid_search = GridSearchCV(estimator= svc,\n","                          param_grid = parameters, scoring = 'accuracy',cv = 10)\n","grid_search = grid_search.fit(xtrain_d2v, ytrain)\n","grid_search.best_params_\n","prediction = grid_search.predict(xvalid_d2v) \n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction)\n","print(report)\n","acc1=accuracy_score(yvalid,prediction)\n","\n","\n","print(\"SVM_RBF(doc2vec) after HT+GS, Accuracy Score:\",acc1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.82      0.95      0.88       172\n","           1       0.72      0.39      0.51        59\n","\n","    accuracy                           0.81       231\n","   macro avg       0.77      0.67      0.69       231\n","weighted avg       0.79      0.81      0.78       231\n","\n","SVM_RBF(doc2vec) after HT+GS, Accuracy Score: 0.8051948051948052\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NiLd5MqjVM4V","outputId":"13a97b41-ddfd-4948-c63b-ef417f036f0f"},"source":["from sklearn.model_selection import RandomizedSearchCV\n","#svc = svm.SVC()\n","\n","# Create the random grid\n","random_grid = {'C': [0.1,1,10,100], \n","              'kernel': ['linear','rbf']}\n","              \n","rf_random = RandomizedSearchCV(estimator = svc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n","# Fit the random search model\n","rf_random.fit(xtrain_d2v, ytrain)\n","print(\"best Params\")\n","print(rf_random.best_params_)\n","\n","\n","prediction = rf_random.predict(xvalid_d2v) \n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction)\n","print(report)\n","acc1=accuracy_score(yvalid,prediction)\n","\n","\n","print(\"SVM_RBF(doc2vec) after HT+RS, Accuracy Score:\",acc1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    3.1s finished\n"],"name":"stderr"},{"output_type":"stream","text":["best Params\n","{'kernel': 'linear', 'C': 100}\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.94      0.88       172\n","           1       0.69      0.42      0.53        59\n","\n","    accuracy                           0.81       231\n","   macro avg       0.76      0.68      0.70       231\n","weighted avg       0.79      0.81      0.79       231\n","\n","SVM_RBF(doc2vec) after HT+RS, Accuracy Score: 0.8051948051948052\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RkjfPgbLVijK","outputId":"40ecdb64-a226-4502-84e9-a90e2dd4d07e"},"source":["from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain) \n","prediction = rf.predict(xvalid_d2v)\n","\n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc9=accuracy_score(yvalid,prediction)\n","\n","print(\"RF, Accuracy Score:\",acc9)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.80      0.95      0.87       172\n","           1       0.68      0.29      0.40        59\n","\n","    accuracy                           0.78       231\n","   macro avg       0.74      0.62      0.64       231\n","weighted avg       0.77      0.78      0.75       231\n","\n","RF, Accuracy Score: 0.7835497835497836\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhIeLdJlV5LP","outputId":"89ba5fcb-4fed-4404-b344-a0ca9b6d18fc"},"source":["from sklearn.model_selection import RandomizedSearchCV\n","rf = RandomForestClassifier()\n","#Number of trees in random forest\n","n_estimators = [int(x) for x in np.linspace(start = 500, stop = 1000, num = 100)]\n","# Number of features to consider at every split\n","max_features = ['auto', 'sqrt']\n","# Maximum number of levels in tree\n","max_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\n","max_depth.append(None)\n","# Minimum number of samples required to split a node\n","min_samples_split = [2, 5, 10]\n","# Minimum number of samples required at each leaf node\n","min_samples_leaf = [1, 2, 4]\n","# Method of selecting samples for training each tree\n","bootstrap = [True]\n","# Create the random grid\n","random_grid = {'n_estimators': n_estimators,\n","               'max_features': max_features,\n","               'max_depth': max_depth,\n","               'min_samples_split': min_samples_split,\n","               'min_samples_leaf': min_samples_leaf,\n","               'bootstrap': bootstrap}\n","rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n","# Fit the random search model\n","rf_random.fit(xtrain_d2v, ytrain)\n","\n","print(\"best Params\")\n","print(rf_random.best_params_)\n","\n","prediction = rf_random.predict(xvalid_d2v)\n","\n","#classifiction Report\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc10=accuracy_score(yvalid,prediction)\n","\n","print(\"RF(word2vec) after HT+RS, Accuracy Score:\",acc10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.9min\n","[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  7.8min\n","[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 15.1min finished\n"],"name":"stderr"},{"output_type":"stream","text":["best Params\n","{'n_estimators': 732, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': None, 'bootstrap': True}\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.96      0.87       172\n","           1       0.72      0.31      0.43        59\n","\n","    accuracy                           0.79       231\n","   macro avg       0.76      0.63      0.65       231\n","weighted avg       0.78      0.79      0.76       231\n","\n","RF(word2vec) after HT+RS, Accuracy Score: 0.7922077922077922\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aun97zhEZ1Wn","outputId":"e11f548e-f5ea-43ae-deea-db55cd5f0324"},"source":["from sklearn.model_selection import GridSearchCV\n","# Create the parameter grid \n","param_grid = {\n","    'bootstrap': [True],\n","    'max_depth': [80, 90, 100, 110],\n","    'max_features': [2, 3],\n","    'min_samples_leaf': [3, 4, 5],\n","    'min_samples_split': [8, 10, 12],\n","    'n_estimators': [100, 200, 300, 1000]\n","}\n","# Create a based model\n","rf = RandomForestClassifier()\n","# Instantiate the grid search model\n","grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,cv = 3, n_jobs = -1, verbose = 2)\n","# Fit the grid search to the data\n","grid_search.fit(xtrain_d2v, ytrain)\n","\n","print(\"best Params\")\n","print(grid_search.best_params_)\n","\n","prediction = grid_search.predict(xvalid_d2v)\n","\n","#classifiction Report\n","\n","report = classification_report( yvalid, prediction )\n","print(report)\n","acc1=accuracy_score(yvalid,prediction)\n","\n","print(\"RF(doc2vec) after HT+GS, Accuracy Score:\",acc1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   24.5s\n","[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.9min\n","[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  4.3min\n","[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  7.7min\n","[Parallel(n_jobs=-1)]: Done 864 out of 864 | elapsed: 10.4min finished\n"],"name":"stderr"},{"output_type":"stream","text":["best Params\n","{'bootstrap': True, 'max_depth': 100, 'max_features': 2, 'min_samples_leaf': 5, 'min_samples_split': 8, 'n_estimators': 100}\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.99      0.87       172\n","           1       0.82      0.15      0.26        59\n","\n","    accuracy                           0.77       231\n","   macro avg       0.80      0.57      0.56       231\n","weighted avg       0.78      0.77      0.71       231\n","\n","RF(doc2vec) after HT+GS, Accuracy Score: 0.7748917748917749\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXcixNL1psnL","outputId":"b98e3b05-c6c9-4bb2-c9eb-041600ce99fd"},"source":["from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.models import Sequential\n","model = Sequential()\n","model.add(Embedding(500, 120, input_length = xtrain_d2v.shape[1]))\n","model.add(SpatialDropout1D(0.4))\n","model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n","print(model.summary())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_4 (Embedding)      (None, 200, 120)          60000     \n","_________________________________________________________________\n","spatial_dropout1d_4 (Spatial (None, 200, 120)          0         \n","_________________________________________________________________\n","lstm_4 (LSTM)                (None, 176)               209088    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 177       \n","=================================================================\n","Total params: 269,265\n","Trainable params: 269,265\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJu71A8vrO4O","outputId":"63a4fe48-df32-4152-96ad-ba78c505d223"},"source":["batch_size=32\n","model.fit(xtrain_d2v, ytrain, epochs = 5, batch_size=batch_size, verbose = 'auto')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","29/29 [==============================] - 31s 881ms/step - loss: 0.0000e+00 - accuracy: 0.2393\n","Epoch 2/5\n","29/29 [==============================] - 25s 874ms/step - loss: 0.0000e+00 - accuracy: 0.2489\n","Epoch 3/5\n","29/29 [==============================] - 25s 873ms/step - loss: 0.0000e+00 - accuracy: 0.2450\n","Epoch 4/5\n","29/29 [==============================] - 25s 865ms/step - loss: 0.0000e+00 - accuracy: 0.2548\n","Epoch 5/5\n","29/29 [==============================] - 25s 864ms/step - loss: 0.0000e+00 - accuracy: 0.2626\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4k8e3tQAtGUS"},"source":[""],"execution_count":null,"outputs":[]}]}